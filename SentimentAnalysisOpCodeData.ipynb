{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "SentimentAnalysisOpCodeData.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nalsadi/MalwareRL/blob/master/SentimentAnalysisOpCodeData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5b2497b3-60ee-7cd0-0625-f103214c0ed4",
        "_uuid": "b34dc51c4c60fc1cc8200129e74e7a025fd0cc42",
        "id": "RPypYlx0DrPW",
        "colab_type": "text"
      },
      "source": [
        "**Created by Peter Nagy February 2017 ** <br/>\n",
        "[Github][1] <br/>\n",
        "[Linkedin](https://www.linkedin.com/in/peternagyjob/) <br/>\n",
        "**Sentiment Analysis:** the process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral.\n",
        "\n",
        "\n",
        "  [1]: https://github.com/nagypeterjob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "39c44f0e-d62c-7e11-a542-4fcd58e21442",
        "_uuid": "4efef6a6c3143fbb6bca5903fc1a764bbbb861c4",
        "id": "7TY_S5LiDrPY",
        "colab_type": "text"
      },
      "source": [
        "As an improvement to my previous [Kernel][1], here I am trying to achieve better results with a Recurrent Neural Network. <br/>\n",
        "You may want to [check out](https://www.kaggle.com/ngyptr/multi-class-classification-with-lstm) my latest kernel on an LSTM multi-class classification problem.\n",
        "\n",
        "  [1]: https://www.kaggle.com/ngyptr/d/crowdflower/first-gop-debate-twitter-sentiment/python-nltk-sentiment-analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "6c53202d-5c34-4859-e7e9-8ef5c7068287",
        "_uuid": "717bb968c36b9325c7d4cae5724a3672e49ff243",
        "trusted": true,
        "id": "C8rCKJZQDrPZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import re\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2bc2702e-d6f4-df5f-b80e-50ab23a6d29e",
        "_uuid": "9b520acffb5cd85d0e1ada968ad0f12cee33a4b5",
        "id": "VJrf_-IGDrPc",
        "colab_type": "text"
      },
      "source": [
        "Only keeping the necessary columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "K3VFewWUDrPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"/content/AllData.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "EUKZhHyzDrPh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data.drop(['Unnamed: 0'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4ZHzwL1vDrPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data.rename(columns={'Text':'text','Label':'sentiment'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "t5ho9SfJDrPn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "8b65375f-44d4-4fc4-86b9-8d99fde9e3d0"
      },
      "source": [
        "data"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ADR ADD LDR ADR ADD LDR ADR ADD LDR ADR ADD L...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ADR ADD LDR ADR ADD LDR ADR ADD LDR ADR ADD L...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ADR ADD LDR ADR ADD LDR ADR ADD LDR ADR ADD L...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ADR ADD LDR ADR ADD LDR ADR ADD LDR ADR ADD L...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ADR ADD LDR ADR ADD LDR ADR ADD LDR ADR ADD L...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>507</th>\n",
              "      <td>STMFD LDR LDRB CMP LDMNEFD LDR LDR LDR CMP BE...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>508</th>\n",
              "      <td>BX BX BX BX BX BX BX BX BX BX BX BX BX BX BX ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>509</th>\n",
              "      <td>BX BX BX BX BX BX BX BX BX BX BX BX BX BX BX ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>510</th>\n",
              "      <td>STMFD LDR LDRB CMP LDMNEFD LDR LDR LDR CMP BE...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>511</th>\n",
              "      <td>BX BX BX BX BX BX BX BX BX BX BX BX BX BX BX ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>512 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text  sentiment\n",
              "0     ADR ADD LDR ADR ADD LDR ADR ADD LDR ADR ADD L...          0\n",
              "1     ADR ADD LDR ADR ADD LDR ADR ADD LDR ADR ADD L...          0\n",
              "2     ADR ADD LDR ADR ADD LDR ADR ADD LDR ADR ADD L...          0\n",
              "3     ADR ADD LDR ADR ADD LDR ADR ADD LDR ADR ADD L...          0\n",
              "4     ADR ADD LDR ADR ADD LDR ADR ADD LDR ADR ADD L...          0\n",
              "..                                                 ...        ...\n",
              "507   STMFD LDR LDRB CMP LDMNEFD LDR LDR LDR CMP BE...          1\n",
              "508   BX BX BX BX BX BX BX BX BX BX BX BX BX BX BX ...          1\n",
              "509   BX BX BX BX BX BX BX BX BX BX BX BX BX BX BX ...          1\n",
              "510   STMFD LDR LDRB CMP LDMNEFD LDR LDR LDR CMP BE...          1\n",
              "511   BX BX BX BX BX BX BX BX BX BX BX BX BX BX BX ...          1\n",
              "\n",
              "[512 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "89c8c923-c0bf-7b35-9ab8-e63f00b74e5a",
        "_uuid": "d2bc3bbd2ea3961c49e6673145a0a7226c160e58",
        "trusted": true,
        "id": "Mj8ieFbuDrPs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data = pd.read_csv('../input/Sentiment.csv')\n",
        "# Keeping only the neccessary columns\n",
        "data = data[['text','sentiment']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "pbCdpfARDrPy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a544682-d470-420e-a330-900a3623a962"
      },
      "source": [
        "pos  = data[ data['sentiment'] == 1]\n",
        "pos.shape[0]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "244"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4c0ec63b-cdf8-8e29-812b-0fbbfcea2929",
        "_uuid": "ff12d183224670f9c4c96fd24581b9924d4dff20",
        "id": "_bdiE6QnDrP2",
        "colab_type": "text"
      },
      "source": [
        "Next, I am dropping the 'Neutral' sentiments as my goal was to only differentiate positive and negative tweets. After that, I am filtering the tweets so only valid texts and words remain.  Then, I define the number of max features as 2000 and use Tokenizer to vectorize and convert text into Sequences so the Network can deal with it as input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "43632d2d-6160-12ce-48b0-e5eb1c207076",
        "_uuid": "d0f8b4542106a279f7398db7285ae5e370b2e813",
        "trusted": true,
        "id": "LaN9vTHXDrP2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "43efe0ef-3426-43b6-d8da-6944cc1a3dc9"
      },
      "source": [
        "#data = data[data.sentiment != \"Neutral\"]\n",
        "data['text'] = data['text'].apply(lambda x: x.lower())\n",
        "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
        "\n",
        "print(data[ data['sentiment'] == 1].size)\n",
        "print(data[ data['sentiment'] == 0].size)\n",
        "\n",
        "for idx,row in data.iterrows():\n",
        "    row[0] = row[0].replace('rt',' ')\n",
        "    \n",
        "max_fatures = 2000\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "tokenizer.fit_on_texts(data['text'].values)\n",
        "X = tokenizer.texts_to_sequences(data['text'].values)\n",
        "X = pad_sequences(X)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "488\n",
            "536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmLhF4iSIdfo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "5b70a646-129c-46a3-8271-ad20d3c3bee7"
      },
      "source": [
        "pd.DataFrame(data=X[1:,-20000:], index=X[1:,0])  # 1st row as the column name"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>19960</th>\n",
              "      <th>19961</th>\n",
              "      <th>19962</th>\n",
              "      <th>19963</th>\n",
              "      <th>19964</th>\n",
              "      <th>19965</th>\n",
              "      <th>19966</th>\n",
              "      <th>19967</th>\n",
              "      <th>19968</th>\n",
              "      <th>19969</th>\n",
              "      <th>19970</th>\n",
              "      <th>19971</th>\n",
              "      <th>19972</th>\n",
              "      <th>19973</th>\n",
              "      <th>19974</th>\n",
              "      <th>19975</th>\n",
              "      <th>19976</th>\n",
              "      <th>19977</th>\n",
              "      <th>19978</th>\n",
              "      <th>19979</th>\n",
              "      <th>19980</th>\n",
              "      <th>19981</th>\n",
              "      <th>19982</th>\n",
              "      <th>19983</th>\n",
              "      <th>19984</th>\n",
              "      <th>19985</th>\n",
              "      <th>19986</th>\n",
              "      <th>19987</th>\n",
              "      <th>19988</th>\n",
              "      <th>19989</th>\n",
              "      <th>19990</th>\n",
              "      <th>19991</th>\n",
              "      <th>19992</th>\n",
              "      <th>19993</th>\n",
              "      <th>19994</th>\n",
              "      <th>19995</th>\n",
              "      <th>19996</th>\n",
              "      <th>19997</th>\n",
              "      <th>19998</th>\n",
              "      <th>19999</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>14</td>\n",
              "      <td>13</td>\n",
              "      <td>4</td>\n",
              "      <td>17</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>58</td>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>14</td>\n",
              "      <td>13</td>\n",
              "      <td>17</td>\n",
              "      <td>14</td>\n",
              "      <td>13</td>\n",
              "      <td>152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>81</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>52</td>\n",
              "      <td>9</td>\n",
              "      <td>54</td>\n",
              "      <td>21</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>17</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>58</td>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>14</td>\n",
              "      <td>13</td>\n",
              "      <td>17</td>\n",
              "      <td>14</td>\n",
              "      <td>13</td>\n",
              "      <td>152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>17</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>58</td>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>14</td>\n",
              "      <td>13</td>\n",
              "      <td>17</td>\n",
              "      <td>14</td>\n",
              "      <td>13</td>\n",
              "      <td>152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>52</td>\n",
              "      <td>43</td>\n",
              "      <td>104</td>\n",
              "      <td>13</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>13</td>\n",
              "      <td>17</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>58</td>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>14</td>\n",
              "      <td>13</td>\n",
              "      <td>17</td>\n",
              "      <td>14</td>\n",
              "      <td>13</td>\n",
              "      <td>152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>14</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>13</td>\n",
              "      <td>17</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>58</td>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>14</td>\n",
              "      <td>13</td>\n",
              "      <td>17</td>\n",
              "      <td>14</td>\n",
              "      <td>13</td>\n",
              "      <td>152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>14</td>\n",
              "      <td>58</td>\n",
              "      <td>51</td>\n",
              "      <td>36</td>\n",
              "      <td>51</td>\n",
              "      <td>93</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>151</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>59</td>\n",
              "      <td>1</td>\n",
              "      <td>59</td>\n",
              "      <td>52</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>59</td>\n",
              "      <td>1</td>\n",
              "      <td>59</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>23</td>\n",
              "      <td>17</td>\n",
              "      <td>10</td>\n",
              "      <td>76</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>23</td>\n",
              "      <td>9</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>95</td>\n",
              "      <td>152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>57</td>\n",
              "      <td>66</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>42</td>\n",
              "      <td>56</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>58</td>\n",
              "      <td>51</td>\n",
              "      <td>36</td>\n",
              "      <td>51</td>\n",
              "      <td>93</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>151</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>52</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>10</td>\n",
              "      <td>95</td>\n",
              "      <td>152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>69</td>\n",
              "      <td>85</td>\n",
              "      <td>13</td>\n",
              "      <td>42</td>\n",
              "      <td>21</td>\n",
              "      <td>79</td>\n",
              "      <td>13</td>\n",
              "      <td>5</td>\n",
              "      <td>56</td>\n",
              "      <td>56</td>\n",
              "      <td>42</td>\n",
              "      <td>5</td>\n",
              "      <td>56</td>\n",
              "      <td>94</td>\n",
              "      <td>5</td>\n",
              "      <td>56</td>\n",
              "      <td>94</td>\n",
              "      <td>5</td>\n",
              "      <td>78</td>\n",
              "      <td>84</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>13</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>17</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>60</td>\n",
              "      <td>10</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>10</td>\n",
              "      <td>95</td>\n",
              "      <td>152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>51</td>\n",
              "      <td>36</td>\n",
              "      <td>51</td>\n",
              "      <td>93</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>151</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>59</td>\n",
              "      <td>1</td>\n",
              "      <td>59</td>\n",
              "      <td>52</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>59</td>\n",
              "      <td>1</td>\n",
              "      <td>59</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>23</td>\n",
              "      <td>17</td>\n",
              "      <td>10</td>\n",
              "      <td>76</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>23</td>\n",
              "      <td>9</td>\n",
              "      <td>14</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>95</td>\n",
              "      <td>152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>57</td>\n",
              "      <td>66</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>42</td>\n",
              "      <td>56</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>58</td>\n",
              "      <td>51</td>\n",
              "      <td>36</td>\n",
              "      <td>51</td>\n",
              "      <td>93</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>151</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>52</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>10</td>\n",
              "      <td>95</td>\n",
              "      <td>152</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>511 rows Ã— 20000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    0      1      2      3      4      ...  19995  19996  19997  19998  19999\n",
              "0       0      0      0      0      0  ...     13     17     14     13    152\n",
              "0       0      0      0      0      0  ...     13     17     14     13    152\n",
              "0       0      0      0      0      0  ...     13     17     14     13    152\n",
              "0       0      0      0      0      0  ...     13     17     14     13    152\n",
              "0       0      0      0      0      0  ...     13     17     14     13    152\n",
              "..    ...    ...    ...    ...    ...  ...    ...    ...    ...    ...    ...\n",
              "0       0      0      0      0      0  ...     17     10      4     95    152\n",
              "0       0      0      0      0      0  ...      1     17     10     95    152\n",
              "0       0      0      0      0      0  ...      1     17     10     95    152\n",
              "0       0      0      0      0      0  ...     17     10      4     95    152\n",
              "0       0      0      0      0      0  ...      1     17     10     95    152\n",
              "\n",
              "[511 rows x 20000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKHtR8x7J66R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=X[0:,-20000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqY_WpYoJ--c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "000178bb-801d-4802-a830-b1f94401baf5"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(512, 210430)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z756ygJ6Elj0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "46adc3f5-97ec-4ffa-b05f-26a8c26f4b95"
      },
      "source": [
        "X[-1][500:800]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9753421e-1303-77d5-b17f-5f25fa08c452",
        "_uuid": "aa7d103e946e631133d86ef3adc73e1a8b1a1e89",
        "id": "hJSJFFCqDrP6",
        "colab_type": "text"
      },
      "source": [
        "Next, I compose the LSTM Network. Note that **embed_dim**, **lstm_out**, **batch_size**, **droupout_x** variables are hyperparameters, their values are somehow intuitive, can be and must be played with in order to achieve good results. Please also note that I am using softmax as activation function. The reason is that our Network is using categorical crossentropy, and softmax is just the right activation method for that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "1ba3cf60-a83c-9c21-05e0-b14303027e93",
        "_uuid": "05cb9ef0ec9e0a4067e3ab7c1bda7b2c1211feda",
        "trusted": false,
        "id": "yRLphT8TDrP7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "6029af84-5952-46c4-b747-e4ab197dc5d5"
      },
      "source": [
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 2000, 128)         256000    \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_1 (Spatial (None, 2000, 128)         0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 196)               254800    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 394       \n",
            "=================================================================\n",
            "Total params: 511,194\n",
            "Trainable params: 511,194\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_M-saAmNZUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del model "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NcdHoA3M61o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "098702d0-8f14-40b3-d921-f12c8f53c810"
      },
      "source": [
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=210430, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(8, activation='sigmoid'))\n",
        "\n",
        "model.add(Dense(2, activation='sigmoid'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 12)                2525172   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 12)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 8)                 104       \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 2)                 18        \n",
            "=================================================================\n",
            "Total params: 2,525,294\n",
            "Trainable params: 2,525,294\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "15f4ee61-47e4-88c4-4b81-98a85237333f",
        "_uuid": "2dae0f3b95a4ba533453c512e573560a8358e162",
        "id": "iSCS_rI5DrP-",
        "colab_type": "text"
      },
      "source": [
        "Hereby I declare the train and test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b35748b8-2353-3db2-e571-5fd22bb93eb0",
        "_uuid": "a380bbfae2d098d407b138fc44622c9913a31c07",
        "trusted": false,
        "id": "bIpO3AxBDrQA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "085c8d31-ad63-4a50-d1ac-3a2b9da19bdc"
      },
      "source": [
        "Y = pd.get_dummies(data['sentiment']).values\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(343, 210430) (343, 2)\n",
            "(169, 210430) (169, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEli4vQkENBm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "f6299c78-2767-4f99-b530-15aedb4f5690"
      },
      "source": [
        "Y"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0],\n",
              "       [1, 0],\n",
              "       [1, 0],\n",
              "       ...,\n",
              "       [0, 1],\n",
              "       [0, 1],\n",
              "       [0, 1]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2a775979-a930-e627-2963-18557d7bf6e6",
        "_uuid": "8799a667a2c0254cb367c193d86e07ee36d91dd7",
        "id": "XU5SNMaXDrQD",
        "colab_type": "text"
      },
      "source": [
        "Here we train the Network. We should run much more than 7 epoch, but I would have to wait forever for kaggle, so it is 7 for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "d5e499ac-2eba-6ff7-8d9a-ff65eb04099b",
        "_uuid": "d0b239912cf67294a9f5af6883bb159c44318fc7",
        "trusted": false,
        "id": "asElfjRxDrQE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2739e02b-ecaf-4628-ad1d-5c0b71f0e56d"
      },
      "source": [
        "batch_size = 32\n",
        "model.fit(X_train, Y_train, epochs = 700, batch_size=batch_size, verbose = 2)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/700\n",
            " - 0s - loss: 0.5258 - acc: 0.7930\n",
            "Epoch 2/700\n",
            " - 0s - loss: 0.5053 - acc: 0.8017\n",
            "Epoch 3/700\n",
            " - 0s - loss: 0.5176 - acc: 0.7726\n",
            "Epoch 4/700\n",
            " - 0s - loss: 0.5288 - acc: 0.7638\n",
            "Epoch 5/700\n",
            " - 0s - loss: 0.5161 - acc: 0.7813\n",
            "Epoch 6/700\n",
            " - 0s - loss: 0.5158 - acc: 0.7784\n",
            "Epoch 7/700\n",
            " - 0s - loss: 0.4844 - acc: 0.7959\n",
            "Epoch 8/700\n",
            " - 0s - loss: 0.4974 - acc: 0.7872\n",
            "Epoch 9/700\n",
            " - 0s - loss: 0.4698 - acc: 0.8076\n",
            "Epoch 10/700\n",
            " - 0s - loss: 0.4899 - acc: 0.7726\n",
            "Epoch 11/700\n",
            " - 0s - loss: 0.4808 - acc: 0.7784\n",
            "Epoch 12/700\n",
            " - 0s - loss: 0.4675 - acc: 0.7959\n",
            "Epoch 13/700\n",
            " - 0s - loss: 0.4531 - acc: 0.8134\n",
            "Epoch 14/700\n",
            " - 0s - loss: 0.4393 - acc: 0.8338\n",
            "Epoch 15/700\n",
            " - 0s - loss: 0.4341 - acc: 0.8280\n",
            "Epoch 16/700\n",
            " - 0s - loss: 0.4427 - acc: 0.8105\n",
            "Epoch 17/700\n",
            " - 0s - loss: 0.3977 - acc: 0.8542\n",
            "Epoch 18/700\n",
            " - 0s - loss: 0.4038 - acc: 0.8397\n",
            "Epoch 19/700\n",
            " - 0s - loss: 0.4391 - acc: 0.8163\n",
            "Epoch 20/700\n",
            " - 0s - loss: 0.4355 - acc: 0.8280\n",
            "Epoch 21/700\n",
            " - 0s - loss: 0.3973 - acc: 0.8426\n",
            "Epoch 22/700\n",
            " - 0s - loss: 0.3905 - acc: 0.8426\n",
            "Epoch 23/700\n",
            " - 0s - loss: 0.3948 - acc: 0.8455\n",
            "Epoch 24/700\n",
            " - 0s - loss: 0.3903 - acc: 0.8484\n",
            "Epoch 25/700\n",
            " - 0s - loss: 0.4044 - acc: 0.8426\n",
            "Epoch 26/700\n",
            " - 0s - loss: 0.3919 - acc: 0.8659\n",
            "Epoch 27/700\n",
            " - 0s - loss: 0.4537 - acc: 0.7930\n",
            "Epoch 28/700\n",
            " - 0s - loss: 0.4214 - acc: 0.8338\n",
            "Epoch 29/700\n",
            " - 0s - loss: 0.3983 - acc: 0.8367\n",
            "Epoch 30/700\n",
            " - 0s - loss: 0.4028 - acc: 0.8338\n",
            "Epoch 31/700\n",
            " - 0s - loss: 0.3689 - acc: 0.8484\n",
            "Epoch 32/700\n",
            " - 0s - loss: 0.4004 - acc: 0.8163\n",
            "Epoch 33/700\n",
            " - 0s - loss: 0.3554 - acc: 0.8630\n",
            "Epoch 34/700\n",
            " - 0s - loss: 0.3346 - acc: 0.8834\n",
            "Epoch 35/700\n",
            " - 0s - loss: 0.3861 - acc: 0.8426\n",
            "Epoch 36/700\n",
            " - 0s - loss: 0.3317 - acc: 0.8863\n",
            "Epoch 37/700\n",
            " - 0s - loss: 0.3708 - acc: 0.8338\n",
            "Epoch 38/700\n",
            " - 0s - loss: 0.3528 - acc: 0.8571\n",
            "Epoch 39/700\n",
            " - 0s - loss: 0.3606 - acc: 0.8484\n",
            "Epoch 40/700\n",
            " - 0s - loss: 0.3404 - acc: 0.8659\n",
            "Epoch 41/700\n",
            " - 0s - loss: 0.3713 - acc: 0.8397\n",
            "Epoch 42/700\n",
            " - 0s - loss: 0.3464 - acc: 0.8571\n",
            "Epoch 43/700\n",
            " - 0s - loss: 0.3599 - acc: 0.8309\n",
            "Epoch 44/700\n",
            " - 0s - loss: 0.3488 - acc: 0.8455\n",
            "Epoch 45/700\n",
            " - 0s - loss: 0.3475 - acc: 0.8513\n",
            "Epoch 46/700\n",
            " - 0s - loss: 0.3213 - acc: 0.8746\n",
            "Epoch 47/700\n",
            " - 0s - loss: 0.3585 - acc: 0.8338\n",
            "Epoch 48/700\n",
            " - 0s - loss: 0.3165 - acc: 0.8805\n",
            "Epoch 49/700\n",
            " - 0s - loss: 0.3199 - acc: 0.8746\n",
            "Epoch 50/700\n",
            " - 0s - loss: 0.3129 - acc: 0.8601\n",
            "Epoch 51/700\n",
            " - 0s - loss: 0.3759 - acc: 0.8338\n",
            "Epoch 52/700\n",
            " - 0s - loss: 0.3343 - acc: 0.8746\n",
            "Epoch 53/700\n",
            " - 0s - loss: 0.3466 - acc: 0.8601\n",
            "Epoch 54/700\n",
            " - 0s - loss: 0.3536 - acc: 0.8601\n",
            "Epoch 55/700\n",
            " - 0s - loss: 0.3477 - acc: 0.8542\n",
            "Epoch 56/700\n",
            " - 0s - loss: 0.3403 - acc: 0.8484\n",
            "Epoch 57/700\n",
            " - 0s - loss: 0.3523 - acc: 0.8484\n",
            "Epoch 58/700\n",
            " - 0s - loss: 0.3357 - acc: 0.8397\n",
            "Epoch 59/700\n",
            " - 0s - loss: 0.3483 - acc: 0.8426\n",
            "Epoch 60/700\n",
            " - 0s - loss: 0.3479 - acc: 0.8601\n",
            "Epoch 61/700\n",
            " - 0s - loss: 0.3306 - acc: 0.8513\n",
            "Epoch 62/700\n",
            " - 0s - loss: 0.3261 - acc: 0.8630\n",
            "Epoch 63/700\n",
            " - 0s - loss: 0.3791 - acc: 0.8105\n",
            "Epoch 64/700\n",
            " - 0s - loss: 0.3696 - acc: 0.8163\n",
            "Epoch 65/700\n",
            " - 0s - loss: 0.3473 - acc: 0.8455\n",
            "Epoch 66/700\n",
            " - 0s - loss: 0.3254 - acc: 0.8513\n",
            "Epoch 67/700\n",
            " - 0s - loss: 0.3381 - acc: 0.8338\n",
            "Epoch 68/700\n",
            " - 0s - loss: 0.3056 - acc: 0.8513\n",
            "Epoch 69/700\n",
            " - 0s - loss: 0.2938 - acc: 0.8484\n",
            "Epoch 70/700\n",
            " - 0s - loss: 0.3459 - acc: 0.8105\n",
            "Epoch 71/700\n",
            " - 0s - loss: 0.3127 - acc: 0.8338\n",
            "Epoch 72/700\n",
            " - 0s - loss: 0.3207 - acc: 0.8455\n",
            "Epoch 73/700\n",
            " - 0s - loss: 0.3287 - acc: 0.8192\n",
            "Epoch 74/700\n",
            " - 0s - loss: 0.3213 - acc: 0.8659\n",
            "Epoch 75/700\n",
            " - 0s - loss: 0.3262 - acc: 0.8834\n",
            "Epoch 76/700\n",
            " - 0s - loss: 0.3120 - acc: 0.8746\n",
            "Epoch 77/700\n",
            " - 0s - loss: 0.2959 - acc: 0.8805\n",
            "Epoch 78/700\n",
            " - 0s - loss: 0.3330 - acc: 0.8746\n",
            "Epoch 79/700\n",
            " - 0s - loss: 0.3227 - acc: 0.8688\n",
            "Epoch 80/700\n",
            " - 0s - loss: 0.3094 - acc: 0.8921\n",
            "Epoch 81/700\n",
            " - 0s - loss: 0.2959 - acc: 0.8776\n",
            "Epoch 82/700\n",
            " - 0s - loss: 0.3342 - acc: 0.8601\n",
            "Epoch 83/700\n",
            " - 0s - loss: 0.3200 - acc: 0.8688\n",
            "Epoch 84/700\n",
            " - 0s - loss: 0.3317 - acc: 0.8688\n",
            "Epoch 85/700\n",
            " - 0s - loss: 0.2813 - acc: 0.8921\n",
            "Epoch 86/700\n",
            " - 0s - loss: 0.3214 - acc: 0.8805\n",
            "Epoch 87/700\n",
            " - 0s - loss: 0.3439 - acc: 0.8601\n",
            "Epoch 88/700\n",
            " - 0s - loss: 0.3171 - acc: 0.8746\n",
            "Epoch 89/700\n",
            " - 0s - loss: 0.3173 - acc: 0.8659\n",
            "Epoch 90/700\n",
            " - 0s - loss: 0.3349 - acc: 0.8659\n",
            "Epoch 91/700\n",
            " - 0s - loss: 0.2989 - acc: 0.8950\n",
            "Epoch 92/700\n",
            " - 0s - loss: 0.3256 - acc: 0.8717\n",
            "Epoch 93/700\n",
            " - 0s - loss: 0.3224 - acc: 0.8746\n",
            "Epoch 94/700\n",
            " - 0s - loss: 0.2982 - acc: 0.8863\n",
            "Epoch 95/700\n",
            " - 0s - loss: 0.2748 - acc: 0.8950\n",
            "Epoch 96/700\n",
            " - 0s - loss: 0.2999 - acc: 0.8863\n",
            "Epoch 97/700\n",
            " - 0s - loss: 0.2653 - acc: 0.8950\n",
            "Epoch 98/700\n",
            " - 0s - loss: 0.2949 - acc: 0.9067\n",
            "Epoch 99/700\n",
            " - 0s - loss: 0.3123 - acc: 0.8776\n",
            "Epoch 100/700\n",
            " - 0s - loss: 0.3066 - acc: 0.8805\n",
            "Epoch 101/700\n",
            " - 0s - loss: 0.3061 - acc: 0.8834\n",
            "Epoch 102/700\n",
            " - 0s - loss: 0.2912 - acc: 0.8863\n",
            "Epoch 103/700\n",
            " - 0s - loss: 0.2565 - acc: 0.9009\n",
            "Epoch 104/700\n",
            " - 0s - loss: 0.2941 - acc: 0.8892\n",
            "Epoch 105/700\n",
            " - 0s - loss: 0.3124 - acc: 0.8892\n",
            "Epoch 106/700\n",
            " - 0s - loss: 0.3002 - acc: 0.8863\n",
            "Epoch 107/700\n",
            " - 0s - loss: 0.2785 - acc: 0.8921\n",
            "Epoch 108/700\n",
            " - 0s - loss: 0.2813 - acc: 0.8950\n",
            "Epoch 109/700\n",
            " - 0s - loss: 0.2804 - acc: 0.9009\n",
            "Epoch 110/700\n",
            " - 0s - loss: 0.2913 - acc: 0.8892\n",
            "Epoch 111/700\n",
            " - 0s - loss: 0.2813 - acc: 0.9009\n",
            "Epoch 112/700\n",
            " - 0s - loss: 0.3205 - acc: 0.8688\n",
            "Epoch 113/700\n",
            " - 0s - loss: 0.3125 - acc: 0.8746\n",
            "Epoch 114/700\n",
            " - 0s - loss: 0.3060 - acc: 0.8659\n",
            "Epoch 115/700\n",
            " - 0s - loss: 0.3040 - acc: 0.8746\n",
            "Epoch 116/700\n",
            " - 0s - loss: 0.3052 - acc: 0.8805\n",
            "Epoch 117/700\n",
            " - 0s - loss: 0.2810 - acc: 0.9067\n",
            "Epoch 118/700\n",
            " - 0s - loss: 0.3100 - acc: 0.8776\n",
            "Epoch 119/700\n",
            " - 0s - loss: 0.2873 - acc: 0.8863\n",
            "Epoch 120/700\n",
            " - 0s - loss: 0.2884 - acc: 0.8834\n",
            "Epoch 121/700\n",
            " - 0s - loss: 0.2915 - acc: 0.8776\n",
            "Epoch 122/700\n",
            " - 0s - loss: 0.2872 - acc: 0.8921\n",
            "Epoch 123/700\n",
            " - 0s - loss: 0.2955 - acc: 0.8834\n",
            "Epoch 124/700\n",
            " - 0s - loss: 0.2778 - acc: 0.8950\n",
            "Epoch 125/700\n",
            " - 0s - loss: 0.2827 - acc: 0.8776\n",
            "Epoch 126/700\n",
            " - 0s - loss: 0.2712 - acc: 0.9125\n",
            "Epoch 127/700\n",
            " - 0s - loss: 0.2424 - acc: 0.9125\n",
            "Epoch 128/700\n",
            " - 0s - loss: 0.2799 - acc: 0.8746\n",
            "Epoch 129/700\n",
            " - 0s - loss: 0.3094 - acc: 0.8863\n",
            "Epoch 130/700\n",
            " - 0s - loss: 0.2771 - acc: 0.8950\n",
            "Epoch 131/700\n",
            " - 0s - loss: 0.3002 - acc: 0.8805\n",
            "Epoch 132/700\n",
            " - 0s - loss: 0.3158 - acc: 0.8717\n",
            "Epoch 133/700\n",
            " - 0s - loss: 0.2842 - acc: 0.8892\n",
            "Epoch 134/700\n",
            " - 0s - loss: 0.2911 - acc: 0.8921\n",
            "Epoch 135/700\n",
            " - 0s - loss: 0.2508 - acc: 0.9067\n",
            "Epoch 136/700\n",
            " - 0s - loss: 0.2806 - acc: 0.8950\n",
            "Epoch 137/700\n",
            " - 0s - loss: 0.2665 - acc: 0.8980\n",
            "Epoch 138/700\n",
            " - 0s - loss: 0.3005 - acc: 0.8805\n",
            "Epoch 139/700\n",
            " - 0s - loss: 0.3004 - acc: 0.8892\n",
            "Epoch 140/700\n",
            " - 0s - loss: 0.2855 - acc: 0.8921\n",
            "Epoch 141/700\n",
            " - 0s - loss: 0.2867 - acc: 0.8863\n",
            "Epoch 142/700\n",
            " - 0s - loss: 0.2712 - acc: 0.8921\n",
            "Epoch 143/700\n",
            " - 0s - loss: 0.2939 - acc: 0.8746\n",
            "Epoch 144/700\n",
            " - 0s - loss: 0.2660 - acc: 0.9038\n",
            "Epoch 145/700\n",
            " - 0s - loss: 0.2653 - acc: 0.9067\n",
            "Epoch 146/700\n",
            " - 0s - loss: 0.2667 - acc: 0.9125\n",
            "Epoch 147/700\n",
            " - 0s - loss: 0.2671 - acc: 0.8950\n",
            "Epoch 148/700\n",
            " - 0s - loss: 0.2966 - acc: 0.8776\n",
            "Epoch 149/700\n",
            " - 0s - loss: 0.2743 - acc: 0.8921\n",
            "Epoch 150/700\n",
            " - 0s - loss: 0.2533 - acc: 0.9184\n",
            "Epoch 151/700\n",
            " - 0s - loss: 0.3093 - acc: 0.8863\n",
            "Epoch 152/700\n",
            " - 0s - loss: 0.2879 - acc: 0.8892\n",
            "Epoch 153/700\n",
            " - 0s - loss: 0.2538 - acc: 0.8950\n",
            "Epoch 154/700\n",
            " - 0s - loss: 0.3065 - acc: 0.8805\n",
            "Epoch 155/700\n",
            " - 0s - loss: 0.2789 - acc: 0.8863\n",
            "Epoch 156/700\n",
            " - 0s - loss: 0.2759 - acc: 0.8892\n",
            "Epoch 157/700\n",
            " - 0s - loss: 0.2753 - acc: 0.8863\n",
            "Epoch 158/700\n",
            " - 0s - loss: 0.2650 - acc: 0.9009\n",
            "Epoch 159/700\n",
            " - 0s - loss: 0.3025 - acc: 0.8746\n",
            "Epoch 160/700\n",
            " - 0s - loss: 0.2694 - acc: 0.8834\n",
            "Epoch 161/700\n",
            " - 0s - loss: 0.2648 - acc: 0.8921\n",
            "Epoch 162/700\n",
            " - 0s - loss: 0.2420 - acc: 0.8950\n",
            "Epoch 163/700\n",
            " - 0s - loss: 0.2614 - acc: 0.8892\n",
            "Epoch 164/700\n",
            " - 0s - loss: 0.3032 - acc: 0.8659\n",
            "Epoch 165/700\n",
            " - 0s - loss: 0.2871 - acc: 0.8805\n",
            "Epoch 166/700\n",
            " - 0s - loss: 0.2648 - acc: 0.8980\n",
            "Epoch 167/700\n",
            " - 0s - loss: 0.3036 - acc: 0.8834\n",
            "Epoch 168/700\n",
            " - 0s - loss: 0.2738 - acc: 0.8950\n",
            "Epoch 169/700\n",
            " - 0s - loss: 0.3063 - acc: 0.8776\n",
            "Epoch 170/700\n",
            " - 0s - loss: 0.2454 - acc: 0.9096\n",
            "Epoch 171/700\n",
            " - 0s - loss: 0.2883 - acc: 0.8834\n",
            "Epoch 172/700\n",
            " - 0s - loss: 0.3105 - acc: 0.8776\n",
            "Epoch 173/700\n",
            " - 0s - loss: 0.2865 - acc: 0.8834\n",
            "Epoch 174/700\n",
            " - 0s - loss: 0.2867 - acc: 0.8950\n",
            "Epoch 175/700\n",
            " - 0s - loss: 0.3033 - acc: 0.8776\n",
            "Epoch 176/700\n",
            " - 0s - loss: 0.2588 - acc: 0.9184\n",
            "Epoch 177/700\n",
            " - 0s - loss: 0.2676 - acc: 0.9096\n",
            "Epoch 178/700\n",
            " - 0s - loss: 0.2940 - acc: 0.8863\n",
            "Epoch 179/700\n",
            " - 0s - loss: 0.3097 - acc: 0.8717\n",
            "Epoch 180/700\n",
            " - 0s - loss: 0.2655 - acc: 0.8950\n",
            "Epoch 181/700\n",
            " - 0s - loss: 0.2468 - acc: 0.9096\n",
            "Epoch 182/700\n",
            " - 0s - loss: 0.2597 - acc: 0.8892\n",
            "Epoch 183/700\n",
            " - 0s - loss: 0.2602 - acc: 0.8980\n",
            "Epoch 184/700\n",
            " - 0s - loss: 0.2921 - acc: 0.8630\n",
            "Epoch 185/700\n",
            " - 0s - loss: 0.2548 - acc: 0.8892\n",
            "Epoch 186/700\n",
            " - 0s - loss: 0.2775 - acc: 0.8892\n",
            "Epoch 187/700\n",
            " - 0s - loss: 0.2752 - acc: 0.8776\n",
            "Epoch 188/700\n",
            " - 0s - loss: 0.2993 - acc: 0.8746\n",
            "Epoch 189/700\n",
            " - 0s - loss: 0.2909 - acc: 0.8688\n",
            "Epoch 190/700\n",
            " - 0s - loss: 0.2790 - acc: 0.8980\n",
            "Epoch 191/700\n",
            " - 0s - loss: 0.3134 - acc: 0.8659\n",
            "Epoch 192/700\n",
            " - 0s - loss: 0.2898 - acc: 0.8892\n",
            "Epoch 193/700\n",
            " - 0s - loss: 0.3054 - acc: 0.8688\n",
            "Epoch 194/700\n",
            " - 0s - loss: 0.2866 - acc: 0.8805\n",
            "Epoch 195/700\n",
            " - 0s - loss: 0.3009 - acc: 0.8746\n",
            "Epoch 196/700\n",
            " - 0s - loss: 0.2537 - acc: 0.9096\n",
            "Epoch 197/700\n",
            " - 0s - loss: 0.2897 - acc: 0.8921\n",
            "Epoch 198/700\n",
            " - 0s - loss: 0.2757 - acc: 0.8950\n",
            "Epoch 199/700\n",
            " - 0s - loss: 0.2931 - acc: 0.8892\n",
            "Epoch 200/700\n",
            " - 0s - loss: 0.2863 - acc: 0.8980\n",
            "Epoch 201/700\n",
            " - 0s - loss: 0.3201 - acc: 0.8688\n",
            "Epoch 202/700\n",
            " - 0s - loss: 0.3008 - acc: 0.8863\n",
            "Epoch 203/700\n",
            " - 0s - loss: 0.3035 - acc: 0.8717\n",
            "Epoch 204/700\n",
            " - 0s - loss: 0.2811 - acc: 0.8921\n",
            "Epoch 205/700\n",
            " - 0s - loss: 0.3290 - acc: 0.8397\n",
            "Epoch 206/700\n",
            " - 0s - loss: 0.3282 - acc: 0.8601\n",
            "Epoch 207/700\n",
            " - 0s - loss: 0.2925 - acc: 0.8717\n",
            "Epoch 208/700\n",
            " - 0s - loss: 0.3874 - acc: 0.8484\n",
            "Epoch 209/700\n",
            " - 0s - loss: 0.3558 - acc: 0.8513\n",
            "Epoch 210/700\n",
            " - 0s - loss: 0.3172 - acc: 0.8688\n",
            "Epoch 211/700\n",
            " - 0s - loss: 0.3100 - acc: 0.8601\n",
            "Epoch 212/700\n",
            " - 0s - loss: 0.3296 - acc: 0.8571\n",
            "Epoch 213/700\n",
            " - 0s - loss: 0.2931 - acc: 0.8805\n",
            "Epoch 214/700\n",
            " - 0s - loss: 0.2828 - acc: 0.8746\n",
            "Epoch 215/700\n",
            " - 0s - loss: 0.2550 - acc: 0.8980\n",
            "Epoch 216/700\n",
            " - 0s - loss: 0.3189 - acc: 0.8630\n",
            "Epoch 217/700\n",
            " - 0s - loss: 0.2880 - acc: 0.8921\n",
            "Epoch 218/700\n",
            " - 0s - loss: 0.2928 - acc: 0.8921\n",
            "Epoch 219/700\n",
            " - 0s - loss: 0.2871 - acc: 0.8892\n",
            "Epoch 220/700\n",
            " - 0s - loss: 0.3040 - acc: 0.8746\n",
            "Epoch 221/700\n",
            " - 0s - loss: 0.2996 - acc: 0.8863\n",
            "Epoch 222/700\n",
            " - 0s - loss: 0.2911 - acc: 0.8776\n",
            "Epoch 223/700\n",
            " - 0s - loss: 0.2731 - acc: 0.8950\n",
            "Epoch 224/700\n",
            " - 0s - loss: 0.2581 - acc: 0.8980\n",
            "Epoch 225/700\n",
            " - 0s - loss: 0.2892 - acc: 0.8834\n",
            "Epoch 226/700\n",
            " - 0s - loss: 0.2931 - acc: 0.8834\n",
            "Epoch 227/700\n",
            " - 0s - loss: 0.2497 - acc: 0.9038\n",
            "Epoch 228/700\n",
            " - 0s - loss: 0.2682 - acc: 0.8863\n",
            "Epoch 229/700\n",
            " - 0s - loss: 0.2704 - acc: 0.8980\n",
            "Epoch 230/700\n",
            " - 0s - loss: 0.2864 - acc: 0.8805\n",
            "Epoch 231/700\n",
            " - 0s - loss: 0.2830 - acc: 0.8921\n",
            "Epoch 232/700\n",
            " - 0s - loss: 0.2727 - acc: 0.8950\n",
            "Epoch 233/700\n",
            " - 0s - loss: 0.2657 - acc: 0.9009\n",
            "Epoch 234/700\n",
            " - 0s - loss: 0.2708 - acc: 0.8950\n",
            "Epoch 235/700\n",
            " - 0s - loss: 0.2805 - acc: 0.8834\n",
            "Epoch 236/700\n",
            " - 0s - loss: 0.2677 - acc: 0.8921\n",
            "Epoch 237/700\n",
            " - 0s - loss: 0.2774 - acc: 0.9038\n",
            "Epoch 238/700\n",
            " - 0s - loss: 0.2684 - acc: 0.8921\n",
            "Epoch 239/700\n",
            " - 0s - loss: 0.2863 - acc: 0.8863\n",
            "Epoch 240/700\n",
            " - 0s - loss: 0.3111 - acc: 0.8630\n",
            "Epoch 241/700\n",
            " - 0s - loss: 0.2932 - acc: 0.8717\n",
            "Epoch 242/700\n",
            " - 0s - loss: 0.2643 - acc: 0.9009\n",
            "Epoch 243/700\n",
            " - 0s - loss: 0.2738 - acc: 0.8950\n",
            "Epoch 244/700\n",
            " - 0s - loss: 0.2653 - acc: 0.9009\n",
            "Epoch 245/700\n",
            " - 0s - loss: 0.2688 - acc: 0.8921\n",
            "Epoch 246/700\n",
            " - 0s - loss: 0.2727 - acc: 0.8980\n",
            "Epoch 247/700\n",
            " - 0s - loss: 0.2725 - acc: 0.8863\n",
            "Epoch 248/700\n",
            " - 0s - loss: 0.2643 - acc: 0.8950\n",
            "Epoch 249/700\n",
            " - 0s - loss: 0.2885 - acc: 0.8805\n",
            "Epoch 250/700\n",
            " - 0s - loss: 0.3032 - acc: 0.8688\n",
            "Epoch 251/700\n",
            " - 0s - loss: 0.2561 - acc: 0.8980\n",
            "Epoch 252/700\n",
            " - 0s - loss: 0.2727 - acc: 0.8950\n",
            "Epoch 253/700\n",
            " - 0s - loss: 0.2889 - acc: 0.8863\n",
            "Epoch 254/700\n",
            " - 0s - loss: 0.2785 - acc: 0.8805\n",
            "Epoch 255/700\n",
            " - 0s - loss: 0.2848 - acc: 0.8805\n",
            "Epoch 256/700\n",
            " - 0s - loss: 0.2991 - acc: 0.8717\n",
            "Epoch 257/700\n",
            " - 0s - loss: 0.2416 - acc: 0.9038\n",
            "Epoch 258/700\n",
            " - 0s - loss: 0.2775 - acc: 0.8863\n",
            "Epoch 259/700\n",
            " - 0s - loss: 0.2675 - acc: 0.8921\n",
            "Epoch 260/700\n",
            " - 0s - loss: 0.2305 - acc: 0.9096\n",
            "Epoch 261/700\n",
            " - 0s - loss: 0.2853 - acc: 0.8834\n",
            "Epoch 262/700\n",
            " - 0s - loss: 0.2552 - acc: 0.8980\n",
            "Epoch 263/700\n",
            " - 0s - loss: 0.2618 - acc: 0.8921\n",
            "Epoch 264/700\n",
            " - 0s - loss: 0.2741 - acc: 0.8892\n",
            "Epoch 265/700\n",
            " - 0s - loss: 0.2853 - acc: 0.8863\n",
            "Epoch 266/700\n",
            " - 0s - loss: 0.2489 - acc: 0.9038\n",
            "Epoch 267/700\n",
            " - 0s - loss: 0.2506 - acc: 0.9038\n",
            "Epoch 268/700\n",
            " - 0s - loss: 0.2649 - acc: 0.9009\n",
            "Epoch 269/700\n",
            " - 0s - loss: 0.2760 - acc: 0.8892\n",
            "Epoch 270/700\n",
            " - 0s - loss: 0.2693 - acc: 0.8921\n",
            "Epoch 271/700\n",
            " - 0s - loss: 0.2693 - acc: 0.8950\n",
            "Epoch 272/700\n",
            " - 0s - loss: 0.2822 - acc: 0.8863\n",
            "Epoch 273/700\n",
            " - 0s - loss: 0.2790 - acc: 0.8863\n",
            "Epoch 274/700\n",
            " - 0s - loss: 0.2519 - acc: 0.9067\n",
            "Epoch 275/700\n",
            " - 0s - loss: 0.2699 - acc: 0.8921\n",
            "Epoch 276/700\n",
            " - 0s - loss: 0.2839 - acc: 0.8805\n",
            "Epoch 277/700\n",
            " - 0s - loss: 0.2438 - acc: 0.9067\n",
            "Epoch 278/700\n",
            " - 0s - loss: 0.2433 - acc: 0.9155\n",
            "Epoch 279/700\n",
            " - 0s - loss: 0.3059 - acc: 0.8863\n",
            "Epoch 280/700\n",
            " - 0s - loss: 0.2613 - acc: 0.8980\n",
            "Epoch 281/700\n",
            " - 0s - loss: 0.2653 - acc: 0.8980\n",
            "Epoch 282/700\n",
            " - 0s - loss: 0.2712 - acc: 0.8892\n",
            "Epoch 283/700\n",
            " - 0s - loss: 0.2616 - acc: 0.8950\n",
            "Epoch 284/700\n",
            " - 0s - loss: 0.2722 - acc: 0.8921\n",
            "Epoch 285/700\n",
            " - 0s - loss: 0.2621 - acc: 0.8921\n",
            "Epoch 286/700\n",
            " - 0s - loss: 0.2779 - acc: 0.8892\n",
            "Epoch 287/700\n",
            " - 0s - loss: 0.2807 - acc: 0.8863\n",
            "Epoch 288/700\n",
            " - 0s - loss: 0.2779 - acc: 0.8834\n",
            "Epoch 289/700\n",
            " - 0s - loss: 0.2369 - acc: 0.9009\n",
            "Epoch 290/700\n",
            " - 0s - loss: 0.2668 - acc: 0.8921\n",
            "Epoch 291/700\n",
            " - 0s - loss: 0.2624 - acc: 0.8863\n",
            "Epoch 292/700\n",
            " - 0s - loss: 0.2876 - acc: 0.8776\n",
            "Epoch 293/700\n",
            " - 0s - loss: 0.2782 - acc: 0.8863\n",
            "Epoch 294/700\n",
            " - 0s - loss: 0.2552 - acc: 0.8892\n",
            "Epoch 295/700\n",
            " - 0s - loss: 0.2460 - acc: 0.9067\n",
            "Epoch 296/700\n",
            " - 0s - loss: 0.2608 - acc: 0.8892\n",
            "Epoch 297/700\n",
            " - 0s - loss: 0.2564 - acc: 0.8980\n",
            "Epoch 298/700\n",
            " - 0s - loss: 0.2680 - acc: 0.9038\n",
            "Epoch 299/700\n",
            " - 0s - loss: 0.2813 - acc: 0.8805\n",
            "Epoch 300/700\n",
            " - 0s - loss: 0.2814 - acc: 0.8805\n",
            "Epoch 301/700\n",
            " - 0s - loss: 0.2482 - acc: 0.8980\n",
            "Epoch 302/700\n",
            " - 0s - loss: 0.2576 - acc: 0.8863\n",
            "Epoch 303/700\n",
            " - 0s - loss: 0.3633 - acc: 0.8630\n",
            "Epoch 304/700\n",
            " - 0s - loss: 0.3354 - acc: 0.8746\n",
            "Epoch 305/700\n",
            " - 0s - loss: 0.3091 - acc: 0.8980\n",
            "Epoch 306/700\n",
            " - 0s - loss: 0.2390 - acc: 0.9155\n",
            "Epoch 307/700\n",
            " - 0s - loss: 0.2798 - acc: 0.8980\n",
            "Epoch 308/700\n",
            " - 0s - loss: 0.2962 - acc: 0.8950\n",
            "Epoch 309/700\n",
            " - 0s - loss: 0.3037 - acc: 0.8921\n",
            "Epoch 310/700\n",
            " - 0s - loss: 0.2818 - acc: 0.8921\n",
            "Epoch 311/700\n",
            " - 0s - loss: 0.2459 - acc: 0.9096\n",
            "Epoch 312/700\n",
            " - 0s - loss: 0.2493 - acc: 0.9125\n",
            "Epoch 313/700\n",
            " - 0s - loss: 0.2878 - acc: 0.8805\n",
            "Epoch 314/700\n",
            " - 0s - loss: 0.2870 - acc: 0.8921\n",
            "Epoch 315/700\n",
            " - 0s - loss: 0.2473 - acc: 0.9038\n",
            "Epoch 316/700\n",
            " - 0s - loss: 0.2475 - acc: 0.9009\n",
            "Epoch 317/700\n",
            " - 0s - loss: 0.2535 - acc: 0.9038\n",
            "Epoch 318/700\n",
            " - 0s - loss: 0.2646 - acc: 0.8950\n",
            "Epoch 319/700\n",
            " - 0s - loss: 0.3028 - acc: 0.8892\n",
            "Epoch 320/700\n",
            " - 0s - loss: 0.2693 - acc: 0.8980\n",
            "Epoch 321/700\n",
            " - 0s - loss: 0.2746 - acc: 0.8950\n",
            "Epoch 322/700\n",
            " - 0s - loss: 0.3066 - acc: 0.8688\n",
            "Epoch 323/700\n",
            " - 0s - loss: 0.2910 - acc: 0.8805\n",
            "Epoch 324/700\n",
            " - 0s - loss: 0.2886 - acc: 0.8746\n",
            "Epoch 325/700\n",
            " - 0s - loss: 0.3137 - acc: 0.8834\n",
            "Epoch 326/700\n",
            " - 0s - loss: 0.2797 - acc: 0.9038\n",
            "Epoch 327/700\n",
            " - 0s - loss: 0.3194 - acc: 0.8805\n",
            "Epoch 328/700\n",
            " - 0s - loss: 0.3035 - acc: 0.8746\n",
            "Epoch 329/700\n",
            " - 0s - loss: 0.3085 - acc: 0.8921\n",
            "Epoch 330/700\n",
            " - 0s - loss: 0.3404 - acc: 0.8717\n",
            "Epoch 331/700\n",
            " - 0s - loss: 0.2575 - acc: 0.9038\n",
            "Epoch 332/700\n",
            " - 0s - loss: 0.2604 - acc: 0.9009\n",
            "Epoch 333/700\n",
            " - 0s - loss: 0.2782 - acc: 0.8980\n",
            "Epoch 334/700\n",
            " - 0s - loss: 0.2567 - acc: 0.8980\n",
            "Epoch 335/700\n",
            " - 0s - loss: 0.2514 - acc: 0.8950\n",
            "Epoch 336/700\n",
            " - 0s - loss: 0.2818 - acc: 0.8892\n",
            "Epoch 337/700\n",
            " - 0s - loss: 0.2570 - acc: 0.8980\n",
            "Epoch 338/700\n",
            " - 0s - loss: 0.2605 - acc: 0.8921\n",
            "Epoch 339/700\n",
            " - 0s - loss: 0.2694 - acc: 0.8980\n",
            "Epoch 340/700\n",
            " - 0s - loss: 0.2415 - acc: 0.9155\n",
            "Epoch 341/700\n",
            " - 0s - loss: 0.2569 - acc: 0.8980\n",
            "Epoch 342/700\n",
            " - 0s - loss: 0.2330 - acc: 0.9067\n",
            "Epoch 343/700\n",
            " - 0s - loss: 0.2602 - acc: 0.8980\n",
            "Epoch 344/700\n",
            " - 0s - loss: 0.2561 - acc: 0.8863\n",
            "Epoch 345/700\n",
            " - 0s - loss: 0.2634 - acc: 0.9009\n",
            "Epoch 346/700\n",
            " - 0s - loss: 0.2346 - acc: 0.9125\n",
            "Epoch 347/700\n",
            " - 0s - loss: 0.2838 - acc: 0.8863\n",
            "Epoch 348/700\n",
            " - 0s - loss: 0.2665 - acc: 0.9038\n",
            "Epoch 349/700\n",
            " - 0s - loss: 0.2916 - acc: 0.8921\n",
            "Epoch 350/700\n",
            " - 0s - loss: 0.2733 - acc: 0.8980\n",
            "Epoch 351/700\n",
            " - 0s - loss: 0.2351 - acc: 0.9155\n",
            "Epoch 352/700\n",
            " - 0s - loss: 0.2744 - acc: 0.8980\n",
            "Epoch 353/700\n",
            " - 0s - loss: 0.2500 - acc: 0.9038\n",
            "Epoch 354/700\n",
            " - 0s - loss: 0.3104 - acc: 0.8776\n",
            "Epoch 355/700\n",
            " - 0s - loss: 0.2597 - acc: 0.8921\n",
            "Epoch 356/700\n",
            " - 0s - loss: 0.2453 - acc: 0.9096\n",
            "Epoch 357/700\n",
            " - 0s - loss: 0.2634 - acc: 0.9096\n",
            "Epoch 358/700\n",
            " - 0s - loss: 0.2659 - acc: 0.8980\n",
            "Epoch 359/700\n",
            " - 0s - loss: 0.2656 - acc: 0.8921\n",
            "Epoch 360/700\n",
            " - 0s - loss: 0.2800 - acc: 0.8892\n",
            "Epoch 361/700\n",
            " - 0s - loss: 0.2260 - acc: 0.9067\n",
            "Epoch 362/700\n",
            " - 0s - loss: 0.2368 - acc: 0.9067\n",
            "Epoch 363/700\n",
            " - 0s - loss: 0.2357 - acc: 0.9096\n",
            "Epoch 364/700\n",
            " - 0s - loss: 0.2683 - acc: 0.9038\n",
            "Epoch 365/700\n",
            " - 0s - loss: 0.2683 - acc: 0.9096\n",
            "Epoch 366/700\n",
            " - 0s - loss: 0.2559 - acc: 0.9067\n",
            "Epoch 367/700\n",
            " - 0s - loss: 0.2610 - acc: 0.9009\n",
            "Epoch 368/700\n",
            " - 0s - loss: 0.2498 - acc: 0.9096\n",
            "Epoch 369/700\n",
            " - 0s - loss: 0.2453 - acc: 0.9096\n",
            "Epoch 370/700\n",
            " - 0s - loss: 0.2838 - acc: 0.8921\n",
            "Epoch 371/700\n",
            " - 0s - loss: 0.2457 - acc: 0.9155\n",
            "Epoch 372/700\n",
            " - 0s - loss: 0.2740 - acc: 0.8950\n",
            "Epoch 373/700\n",
            " - 0s - loss: 0.2549 - acc: 0.9067\n",
            "Epoch 374/700\n",
            " - 0s - loss: 0.2878 - acc: 0.8921\n",
            "Epoch 375/700\n",
            " - 0s - loss: 0.2946 - acc: 0.8921\n",
            "Epoch 376/700\n",
            " - 0s - loss: 0.2673 - acc: 0.8980\n",
            "Epoch 377/700\n",
            " - 0s - loss: 0.3016 - acc: 0.8863\n",
            "Epoch 378/700\n",
            " - 0s - loss: 0.2634 - acc: 0.9096\n",
            "Epoch 379/700\n",
            " - 0s - loss: 0.2845 - acc: 0.9009\n",
            "Epoch 380/700\n",
            " - 0s - loss: 0.2412 - acc: 0.9067\n",
            "Epoch 381/700\n",
            " - 0s - loss: 0.3235 - acc: 0.8863\n",
            "Epoch 382/700\n",
            " - 0s - loss: 0.3462 - acc: 0.8805\n",
            "Epoch 383/700\n",
            " - 0s - loss: 0.3410 - acc: 0.8630\n",
            "Epoch 384/700\n",
            " - 0s - loss: 0.2646 - acc: 0.9067\n",
            "Epoch 385/700\n",
            " - 0s - loss: 0.2888 - acc: 0.8863\n",
            "Epoch 386/700\n",
            " - 0s - loss: 0.2657 - acc: 0.9125\n",
            "Epoch 387/700\n",
            " - 0s - loss: 0.2894 - acc: 0.9009\n",
            "Epoch 388/700\n",
            " - 0s - loss: 0.2655 - acc: 0.9096\n",
            "Epoch 389/700\n",
            " - 0s - loss: 0.2888 - acc: 0.9009\n",
            "Epoch 390/700\n",
            " - 0s - loss: 0.3271 - acc: 0.8805\n",
            "Epoch 391/700\n",
            " - 0s - loss: 0.2547 - acc: 0.9125\n",
            "Epoch 392/700\n",
            " - 0s - loss: 0.2649 - acc: 0.8921\n",
            "Epoch 393/700\n",
            " - 0s - loss: 0.2470 - acc: 0.9067\n",
            "Epoch 394/700\n",
            " - 0s - loss: 0.2596 - acc: 0.8980\n",
            "Epoch 395/700\n",
            " - 0s - loss: 0.2596 - acc: 0.9096\n",
            "Epoch 396/700\n",
            " - 0s - loss: 0.2531 - acc: 0.9038\n",
            "Epoch 397/700\n",
            " - 0s - loss: 0.2527 - acc: 0.9067\n",
            "Epoch 398/700\n",
            " - 0s - loss: 0.2663 - acc: 0.8980\n",
            "Epoch 399/700\n",
            " - 0s - loss: 0.2476 - acc: 0.9125\n",
            "Epoch 400/700\n",
            " - 0s - loss: 0.2341 - acc: 0.9125\n",
            "Epoch 401/700\n",
            " - 0s - loss: 0.2882 - acc: 0.8863\n",
            "Epoch 402/700\n",
            " - 0s - loss: 0.2351 - acc: 0.9125\n",
            "Epoch 403/700\n",
            " - 0s - loss: 0.2588 - acc: 0.9038\n",
            "Epoch 404/700\n",
            " - 0s - loss: 0.2995 - acc: 0.8921\n",
            "Epoch 405/700\n",
            " - 0s - loss: 0.2818 - acc: 0.8863\n",
            "Epoch 406/700\n",
            " - 0s - loss: 0.2682 - acc: 0.9009\n",
            "Epoch 407/700\n",
            " - 0s - loss: 0.2381 - acc: 0.9125\n",
            "Epoch 408/700\n",
            " - 0s - loss: 0.2616 - acc: 0.9009\n",
            "Epoch 409/700\n",
            " - 0s - loss: 0.2652 - acc: 0.9009\n",
            "Epoch 410/700\n",
            " - 0s - loss: 0.2471 - acc: 0.9038\n",
            "Epoch 411/700\n",
            " - 0s - loss: 0.2270 - acc: 0.9184\n",
            "Epoch 412/700\n",
            " - 0s - loss: 0.2276 - acc: 0.9067\n",
            "Epoch 413/700\n",
            " - 0s - loss: 0.2613 - acc: 0.8980\n",
            "Epoch 414/700\n",
            " - 0s - loss: 0.2815 - acc: 0.8892\n",
            "Epoch 415/700\n",
            " - 0s - loss: 0.2347 - acc: 0.9038\n",
            "Epoch 416/700\n",
            " - 0s - loss: 0.2512 - acc: 0.9125\n",
            "Epoch 417/700\n",
            " - 0s - loss: 0.2562 - acc: 0.8950\n",
            "Epoch 418/700\n",
            " - 0s - loss: 0.2250 - acc: 0.9155\n",
            "Epoch 419/700\n",
            " - 0s - loss: 0.2449 - acc: 0.9038\n",
            "Epoch 420/700\n",
            " - 0s - loss: 0.2453 - acc: 0.9125\n",
            "Epoch 421/700\n",
            " - 0s - loss: 0.2156 - acc: 0.9271\n",
            "Epoch 422/700\n",
            " - 0s - loss: 0.2930 - acc: 0.8776\n",
            "Epoch 423/700\n",
            " - 0s - loss: 0.2416 - acc: 0.9009\n",
            "Epoch 424/700\n",
            " - 0s - loss: 0.2689 - acc: 0.8863\n",
            "Epoch 425/700\n",
            " - 0s - loss: 0.2621 - acc: 0.9038\n",
            "Epoch 426/700\n",
            " - 0s - loss: 0.2127 - acc: 0.9271\n",
            "Epoch 427/700\n",
            " - 0s - loss: 0.2687 - acc: 0.8892\n",
            "Epoch 428/700\n",
            " - 0s - loss: 0.2508 - acc: 0.8921\n",
            "Epoch 429/700\n",
            " - 0s - loss: 0.2620 - acc: 0.8980\n",
            "Epoch 430/700\n",
            " - 0s - loss: 0.2278 - acc: 0.9271\n",
            "Epoch 431/700\n",
            " - 0s - loss: 0.2407 - acc: 0.9125\n",
            "Epoch 432/700\n",
            " - 0s - loss: 0.2748 - acc: 0.8892\n",
            "Epoch 433/700\n",
            " - 0s - loss: 0.2296 - acc: 0.9067\n",
            "Epoch 434/700\n",
            " - 0s - loss: 0.3080 - acc: 0.8746\n",
            "Epoch 435/700\n",
            " - 0s - loss: 0.2247 - acc: 0.9155\n",
            "Epoch 436/700\n",
            " - 0s - loss: 0.2575 - acc: 0.8950\n",
            "Epoch 437/700\n",
            " - 0s - loss: 0.2494 - acc: 0.9096\n",
            "Epoch 438/700\n",
            " - 0s - loss: 0.2553 - acc: 0.8950\n",
            "Epoch 439/700\n",
            " - 0s - loss: 0.2597 - acc: 0.8980\n",
            "Epoch 440/700\n",
            " - 0s - loss: 0.2763 - acc: 0.8834\n",
            "Epoch 441/700\n",
            " - 0s - loss: 0.2462 - acc: 0.9038\n",
            "Epoch 442/700\n",
            " - 0s - loss: 0.2438 - acc: 0.9038\n",
            "Epoch 443/700\n",
            " - 0s - loss: 0.2124 - acc: 0.9155\n",
            "Epoch 444/700\n",
            " - 0s - loss: 0.2635 - acc: 0.8805\n",
            "Epoch 445/700\n",
            " - 0s - loss: 0.2590 - acc: 0.8980\n",
            "Epoch 446/700\n",
            " - 0s - loss: 0.2203 - acc: 0.9155\n",
            "Epoch 447/700\n",
            " - 0s - loss: 0.2467 - acc: 0.8950\n",
            "Epoch 448/700\n",
            " - 0s - loss: 0.2553 - acc: 0.8950\n",
            "Epoch 449/700\n",
            " - 0s - loss: 0.2806 - acc: 0.8950\n",
            "Epoch 450/700\n",
            " - 0s - loss: 0.2412 - acc: 0.9038\n",
            "Epoch 451/700\n",
            " - 0s - loss: 0.2335 - acc: 0.9155\n",
            "Epoch 452/700\n",
            " - 0s - loss: 0.2412 - acc: 0.9096\n",
            "Epoch 453/700\n",
            " - 0s - loss: 0.2498 - acc: 0.9155\n",
            "Epoch 454/700\n",
            " - 0s - loss: 0.2609 - acc: 0.8950\n",
            "Epoch 455/700\n",
            " - 0s - loss: 0.2390 - acc: 0.9125\n",
            "Epoch 456/700\n",
            " - 0s - loss: 0.2729 - acc: 0.8950\n",
            "Epoch 457/700\n",
            " - 0s - loss: 0.2201 - acc: 0.9155\n",
            "Epoch 458/700\n",
            " - 0s - loss: 0.2765 - acc: 0.8921\n",
            "Epoch 459/700\n",
            " - 0s - loss: 0.2464 - acc: 0.9067\n",
            "Epoch 460/700\n",
            " - 0s - loss: 0.2886 - acc: 0.8892\n",
            "Epoch 461/700\n",
            " - 0s - loss: 0.2330 - acc: 0.9067\n",
            "Epoch 462/700\n",
            " - 0s - loss: 0.2489 - acc: 0.8950\n",
            "Epoch 463/700\n",
            " - 0s - loss: 0.2425 - acc: 0.9009\n",
            "Epoch 464/700\n",
            " - 0s - loss: 0.2572 - acc: 0.8892\n",
            "Epoch 465/700\n",
            " - 0s - loss: 0.2364 - acc: 0.9038\n",
            "Epoch 466/700\n",
            " - 0s - loss: 0.2429 - acc: 0.9096\n",
            "Epoch 467/700\n",
            " - 0s - loss: 0.2313 - acc: 0.9067\n",
            "Epoch 468/700\n",
            " - 0s - loss: 0.2311 - acc: 0.9038\n",
            "Epoch 469/700\n",
            " - 0s - loss: 0.2685 - acc: 0.9009\n",
            "Epoch 470/700\n",
            " - 0s - loss: 0.2427 - acc: 0.9009\n",
            "Epoch 471/700\n",
            " - 0s - loss: 0.2341 - acc: 0.9067\n",
            "Epoch 472/700\n",
            " - 0s - loss: 0.2669 - acc: 0.8863\n",
            "Epoch 473/700\n",
            " - 0s - loss: 0.2631 - acc: 0.8950\n",
            "Epoch 474/700\n",
            " - 0s - loss: 0.2634 - acc: 0.8921\n",
            "Epoch 475/700\n",
            " - 0s - loss: 0.2520 - acc: 0.8980\n",
            "Epoch 476/700\n",
            " - 0s - loss: 0.2534 - acc: 0.8980\n",
            "Epoch 477/700\n",
            " - 0s - loss: 0.2536 - acc: 0.8834\n",
            "Epoch 478/700\n",
            " - 0s - loss: 0.2470 - acc: 0.8980\n",
            "Epoch 479/700\n",
            " - 0s - loss: 0.2438 - acc: 0.9009\n",
            "Epoch 480/700\n",
            " - 0s - loss: 0.2209 - acc: 0.9125\n",
            "Epoch 481/700\n",
            " - 0s - loss: 0.2498 - acc: 0.9038\n",
            "Epoch 482/700\n",
            " - 0s - loss: 0.2385 - acc: 0.9009\n",
            "Epoch 483/700\n",
            " - 0s - loss: 0.2591 - acc: 0.8980\n",
            "Epoch 484/700\n",
            " - 0s - loss: 0.2461 - acc: 0.9009\n",
            "Epoch 485/700\n",
            " - 0s - loss: 0.2324 - acc: 0.9067\n",
            "Epoch 486/700\n",
            " - 0s - loss: 0.2626 - acc: 0.8863\n",
            "Epoch 487/700\n",
            " - 0s - loss: 0.2407 - acc: 0.9125\n",
            "Epoch 488/700\n",
            " - 0s - loss: 0.2668 - acc: 0.8892\n",
            "Epoch 489/700\n",
            " - 0s - loss: 0.2264 - acc: 0.9155\n",
            "Epoch 490/700\n",
            " - 0s - loss: 0.2358 - acc: 0.9009\n",
            "Epoch 491/700\n",
            " - 0s - loss: 0.2700 - acc: 0.8805\n",
            "Epoch 492/700\n",
            " - 0s - loss: 0.2330 - acc: 0.9038\n",
            "Epoch 493/700\n",
            " - 0s - loss: 0.2414 - acc: 0.9067\n",
            "Epoch 494/700\n",
            " - 0s - loss: 0.2165 - acc: 0.9155\n",
            "Epoch 495/700\n",
            " - 0s - loss: 0.2755 - acc: 0.8863\n",
            "Epoch 496/700\n",
            " - 0s - loss: 0.2428 - acc: 0.8980\n",
            "Epoch 497/700\n",
            " - 0s - loss: 0.2475 - acc: 0.9096\n",
            "Epoch 498/700\n",
            " - 0s - loss: 0.2746 - acc: 0.8950\n",
            "Epoch 499/700\n",
            " - 0s - loss: 0.2625 - acc: 0.8834\n",
            "Epoch 500/700\n",
            " - 0s - loss: 0.2309 - acc: 0.9184\n",
            "Epoch 501/700\n",
            " - 0s - loss: 0.2477 - acc: 0.9038\n",
            "Epoch 502/700\n",
            " - 0s - loss: 0.2588 - acc: 0.8980\n",
            "Epoch 503/700\n",
            " - 0s - loss: 0.2911 - acc: 0.8776\n",
            "Epoch 504/700\n",
            " - 0s - loss: 0.2413 - acc: 0.9038\n",
            "Epoch 505/700\n",
            " - 0s - loss: 0.2617 - acc: 0.8980\n",
            "Epoch 506/700\n",
            " - 0s - loss: 0.2331 - acc: 0.9009\n",
            "Epoch 507/700\n",
            " - 0s - loss: 0.2330 - acc: 0.9155\n",
            "Epoch 508/700\n",
            " - 0s - loss: 0.2247 - acc: 0.9184\n",
            "Epoch 509/700\n",
            " - 0s - loss: 0.2472 - acc: 0.9038\n",
            "Epoch 510/700\n",
            " - 0s - loss: 0.2419 - acc: 0.9096\n",
            "Epoch 511/700\n",
            " - 0s - loss: 0.2388 - acc: 0.9096\n",
            "Epoch 512/700\n",
            " - 0s - loss: 0.2401 - acc: 0.9009\n",
            "Epoch 513/700\n",
            " - 0s - loss: 0.2211 - acc: 0.9096\n",
            "Epoch 514/700\n",
            " - 0s - loss: 0.2316 - acc: 0.9096\n",
            "Epoch 515/700\n",
            " - 0s - loss: 0.2505 - acc: 0.9038\n",
            "Epoch 516/700\n",
            " - 0s - loss: 0.2428 - acc: 0.9096\n",
            "Epoch 517/700\n",
            " - 0s - loss: 0.2471 - acc: 0.8950\n",
            "Epoch 518/700\n",
            " - 0s - loss: 0.2817 - acc: 0.8805\n",
            "Epoch 519/700\n",
            " - 0s - loss: 0.2574 - acc: 0.8980\n",
            "Epoch 520/700\n",
            " - 0s - loss: 0.2650 - acc: 0.9009\n",
            "Epoch 521/700\n",
            " - 0s - loss: 0.2295 - acc: 0.9009\n",
            "Epoch 522/700\n",
            " - 0s - loss: 0.2557 - acc: 0.8921\n",
            "Epoch 523/700\n",
            " - 0s - loss: 0.2169 - acc: 0.9155\n",
            "Epoch 524/700\n",
            " - 0s - loss: 0.2356 - acc: 0.9096\n",
            "Epoch 525/700\n",
            " - 0s - loss: 0.2278 - acc: 0.9038\n",
            "Epoch 526/700\n",
            " - 0s - loss: 0.2199 - acc: 0.9096\n",
            "Epoch 527/700\n",
            " - 0s - loss: 0.2606 - acc: 0.8921\n",
            "Epoch 528/700\n",
            " - 0s - loss: 0.2207 - acc: 0.9125\n",
            "Epoch 529/700\n",
            " - 0s - loss: 0.2527 - acc: 0.8892\n",
            "Epoch 530/700\n",
            " - 0s - loss: 0.2359 - acc: 0.9155\n",
            "Epoch 531/700\n",
            " - 0s - loss: 0.2385 - acc: 0.9067\n",
            "Epoch 532/700\n",
            " - 0s - loss: 0.2521 - acc: 0.9038\n",
            "Epoch 533/700\n",
            " - 0s - loss: 0.2241 - acc: 0.9184\n",
            "Epoch 534/700\n",
            " - 0s - loss: 0.2495 - acc: 0.9096\n",
            "Epoch 535/700\n",
            " - 0s - loss: 0.2497 - acc: 0.8980\n",
            "Epoch 536/700\n",
            " - 0s - loss: 0.2646 - acc: 0.8921\n",
            "Epoch 537/700\n",
            " - 0s - loss: 0.2709 - acc: 0.8863\n",
            "Epoch 538/700\n",
            " - 0s - loss: 0.2780 - acc: 0.8921\n",
            "Epoch 539/700\n",
            " - 0s - loss: 0.2497 - acc: 0.8921\n",
            "Epoch 540/700\n",
            " - 0s - loss: 0.2662 - acc: 0.9038\n",
            "Epoch 541/700\n",
            " - 0s - loss: 0.2522 - acc: 0.9038\n",
            "Epoch 542/700\n",
            " - 0s - loss: 0.2449 - acc: 0.8921\n",
            "Epoch 543/700\n",
            " - 0s - loss: 0.2513 - acc: 0.8921\n",
            "Epoch 544/700\n",
            " - 0s - loss: 0.2239 - acc: 0.9038\n",
            "Epoch 545/700\n",
            " - 0s - loss: 0.2233 - acc: 0.9125\n",
            "Epoch 546/700\n",
            " - 0s - loss: 0.2869 - acc: 0.8863\n",
            "Epoch 547/700\n",
            " - 0s - loss: 0.2256 - acc: 0.9155\n",
            "Epoch 548/700\n",
            " - 0s - loss: 0.2755 - acc: 0.8834\n",
            "Epoch 549/700\n",
            " - 0s - loss: 0.2343 - acc: 0.9038\n",
            "Epoch 550/700\n",
            " - 0s - loss: 0.2288 - acc: 0.9155\n",
            "Epoch 551/700\n",
            " - 0s - loss: 0.2334 - acc: 0.9125\n",
            "Epoch 552/700\n",
            " - 0s - loss: 0.2405 - acc: 0.9067\n",
            "Epoch 553/700\n",
            " - 0s - loss: 0.2559 - acc: 0.8980\n",
            "Epoch 554/700\n",
            " - 0s - loss: 0.2738 - acc: 0.8863\n",
            "Epoch 555/700\n",
            " - 0s - loss: 0.2785 - acc: 0.8892\n",
            "Epoch 556/700\n",
            " - 0s - loss: 0.2907 - acc: 0.8834\n",
            "Epoch 557/700\n",
            " - 0s - loss: 0.2524 - acc: 0.9009\n",
            "Epoch 558/700\n",
            " - 0s - loss: 0.2728 - acc: 0.8892\n",
            "Epoch 559/700\n",
            " - 0s - loss: 0.2238 - acc: 0.9009\n",
            "Epoch 560/700\n",
            " - 0s - loss: 0.2178 - acc: 0.9155\n",
            "Epoch 561/700\n",
            " - 0s - loss: 0.2673 - acc: 0.8950\n",
            "Epoch 562/700\n",
            " - 0s - loss: 0.2592 - acc: 0.8805\n",
            "Epoch 563/700\n",
            " - 0s - loss: 0.2348 - acc: 0.9096\n",
            "Epoch 564/700\n",
            " - 0s - loss: 0.2545 - acc: 0.8921\n",
            "Epoch 565/700\n",
            " - 0s - loss: 0.2150 - acc: 0.9184\n",
            "Epoch 566/700\n",
            " - 0s - loss: 0.2323 - acc: 0.9125\n",
            "Epoch 567/700\n",
            " - 0s - loss: 0.2794 - acc: 0.8863\n",
            "Epoch 568/700\n",
            " - 0s - loss: 0.2340 - acc: 0.9213\n",
            "Epoch 569/700\n",
            " - 0s - loss: 0.2324 - acc: 0.9096\n",
            "Epoch 570/700\n",
            " - 0s - loss: 0.2475 - acc: 0.9096\n",
            "Epoch 571/700\n",
            " - 0s - loss: 0.2516 - acc: 0.9009\n",
            "Epoch 572/700\n",
            " - 0s - loss: 0.2068 - acc: 0.9242\n",
            "Epoch 573/700\n",
            " - 0s - loss: 0.2660 - acc: 0.8892\n",
            "Epoch 574/700\n",
            " - 0s - loss: 0.2387 - acc: 0.8980\n",
            "Epoch 575/700\n",
            " - 0s - loss: 0.2264 - acc: 0.9184\n",
            "Epoch 576/700\n",
            " - 0s - loss: 0.2324 - acc: 0.9096\n",
            "Epoch 577/700\n",
            " - 0s - loss: 0.2605 - acc: 0.8950\n",
            "Epoch 578/700\n",
            " - 0s - loss: 0.2464 - acc: 0.8980\n",
            "Epoch 579/700\n",
            " - 0s - loss: 0.2197 - acc: 0.9213\n",
            "Epoch 580/700\n",
            " - 0s - loss: 0.2611 - acc: 0.8950\n",
            "Epoch 581/700\n",
            " - 0s - loss: 0.2263 - acc: 0.9096\n",
            "Epoch 582/700\n",
            " - 0s - loss: 0.2492 - acc: 0.9009\n",
            "Epoch 583/700\n",
            " - 0s - loss: 0.2339 - acc: 0.9009\n",
            "Epoch 584/700\n",
            " - 0s - loss: 0.2515 - acc: 0.9009\n",
            "Epoch 585/700\n",
            " - 0s - loss: 0.2356 - acc: 0.9213\n",
            "Epoch 586/700\n",
            " - 0s - loss: 0.2698 - acc: 0.9038\n",
            "Epoch 587/700\n",
            " - 0s - loss: 0.2462 - acc: 0.9067\n",
            "Epoch 588/700\n",
            " - 0s - loss: 0.2210 - acc: 0.9125\n",
            "Epoch 589/700\n",
            " - 0s - loss: 0.2385 - acc: 0.9096\n",
            "Epoch 590/700\n",
            " - 0s - loss: 0.2602 - acc: 0.8921\n",
            "Epoch 591/700\n",
            " - 0s - loss: 0.2279 - acc: 0.9125\n",
            "Epoch 592/700\n",
            " - 0s - loss: 0.2291 - acc: 0.9038\n",
            "Epoch 593/700\n",
            " - 0s - loss: 0.2284 - acc: 0.9096\n",
            "Epoch 594/700\n",
            " - 0s - loss: 0.2393 - acc: 0.9067\n",
            "Epoch 595/700\n",
            " - 0s - loss: 0.2362 - acc: 0.9096\n",
            "Epoch 596/700\n",
            " - 0s - loss: 0.2774 - acc: 0.8950\n",
            "Epoch 597/700\n",
            " - 0s - loss: 0.2515 - acc: 0.8950\n",
            "Epoch 598/700\n",
            " - 0s - loss: 0.2400 - acc: 0.9038\n",
            "Epoch 599/700\n",
            " - 0s - loss: 0.2433 - acc: 0.9038\n",
            "Epoch 600/700\n",
            " - 0s - loss: 0.2510 - acc: 0.8892\n",
            "Epoch 601/700\n",
            " - 0s - loss: 0.2632 - acc: 0.8980\n",
            "Epoch 602/700\n",
            " - 0s - loss: 0.2396 - acc: 0.9096\n",
            "Epoch 603/700\n",
            " - 0s - loss: 0.2273 - acc: 0.9184\n",
            "Epoch 604/700\n",
            " - 0s - loss: 0.2423 - acc: 0.9096\n",
            "Epoch 605/700\n",
            " - 0s - loss: 0.2089 - acc: 0.9242\n",
            "Epoch 606/700\n",
            " - 0s - loss: 0.2406 - acc: 0.9096\n",
            "Epoch 607/700\n",
            " - 0s - loss: 0.2508 - acc: 0.9067\n",
            "Epoch 608/700\n",
            " - 0s - loss: 0.2269 - acc: 0.9067\n",
            "Epoch 609/700\n",
            " - 0s - loss: 0.2434 - acc: 0.9067\n",
            "Epoch 610/700\n",
            " - 0s - loss: 0.2367 - acc: 0.9009\n",
            "Epoch 611/700\n",
            " - 0s - loss: 0.2330 - acc: 0.9067\n",
            "Epoch 612/700\n",
            " - 0s - loss: 0.2442 - acc: 0.9009\n",
            "Epoch 613/700\n",
            " - 0s - loss: 0.2266 - acc: 0.9096\n",
            "Epoch 614/700\n",
            " - 0s - loss: 0.2762 - acc: 0.8688\n",
            "Epoch 615/700\n",
            " - 0s - loss: 0.2364 - acc: 0.9125\n",
            "Epoch 616/700\n",
            " - 0s - loss: 0.2282 - acc: 0.9038\n",
            "Epoch 617/700\n",
            " - 0s - loss: 0.2388 - acc: 0.9009\n",
            "Epoch 618/700\n",
            " - 0s - loss: 0.2687 - acc: 0.8921\n",
            "Epoch 619/700\n",
            " - 0s - loss: 0.2454 - acc: 0.9067\n",
            "Epoch 620/700\n",
            " - 0s - loss: 0.2321 - acc: 0.9125\n",
            "Epoch 621/700\n",
            " - 0s - loss: 0.2579 - acc: 0.9038\n",
            "Epoch 622/700\n",
            " - 0s - loss: 0.2507 - acc: 0.8950\n",
            "Epoch 623/700\n",
            " - 0s - loss: 0.2363 - acc: 0.8980\n",
            "Epoch 624/700\n",
            " - 0s - loss: 0.2474 - acc: 0.8980\n",
            "Epoch 625/700\n",
            " - 0s - loss: 0.2109 - acc: 0.9213\n",
            "Epoch 626/700\n",
            " - 0s - loss: 0.2298 - acc: 0.9155\n",
            "Epoch 627/700\n",
            " - 0s - loss: 0.2599 - acc: 0.8921\n",
            "Epoch 628/700\n",
            " - 0s - loss: 0.2253 - acc: 0.9125\n",
            "Epoch 629/700\n",
            " - 0s - loss: 0.2480 - acc: 0.9067\n",
            "Epoch 630/700\n",
            " - 0s - loss: 0.2459 - acc: 0.9096\n",
            "Epoch 631/700\n",
            " - 0s - loss: 0.2807 - acc: 0.8805\n",
            "Epoch 632/700\n",
            " - 0s - loss: 0.2308 - acc: 0.9038\n",
            "Epoch 633/700\n",
            " - 0s - loss: 0.2883 - acc: 0.8892\n",
            "Epoch 634/700\n",
            " - 0s - loss: 0.2642 - acc: 0.8921\n",
            "Epoch 635/700\n",
            " - 0s - loss: 0.2360 - acc: 0.9096\n",
            "Epoch 636/700\n",
            " - 0s - loss: 0.2154 - acc: 0.9242\n",
            "Epoch 637/700\n",
            " - 0s - loss: 0.2189 - acc: 0.9155\n",
            "Epoch 638/700\n",
            " - 0s - loss: 0.2651 - acc: 0.8834\n",
            "Epoch 639/700\n",
            " - 0s - loss: 0.2320 - acc: 0.9067\n",
            "Epoch 640/700\n",
            " - 0s - loss: 0.2619 - acc: 0.9009\n",
            "Epoch 641/700\n",
            " - 0s - loss: 0.2285 - acc: 0.9125\n",
            "Epoch 642/700\n",
            " - 0s - loss: 0.2324 - acc: 0.9038\n",
            "Epoch 643/700\n",
            " - 0s - loss: 0.2370 - acc: 0.9125\n",
            "Epoch 644/700\n",
            " - 0s - loss: 0.2401 - acc: 0.9009\n",
            "Epoch 645/700\n",
            " - 0s - loss: 0.2625 - acc: 0.8921\n",
            "Epoch 646/700\n",
            " - 0s - loss: 0.2491 - acc: 0.9038\n",
            "Epoch 647/700\n",
            " - 0s - loss: 0.2569 - acc: 0.8980\n",
            "Epoch 648/700\n",
            " - 0s - loss: 0.2535 - acc: 0.9096\n",
            "Epoch 649/700\n",
            " - 0s - loss: 0.2263 - acc: 0.9067\n",
            "Epoch 650/700\n",
            " - 0s - loss: 0.2395 - acc: 0.9038\n",
            "Epoch 651/700\n",
            " - 0s - loss: 0.2415 - acc: 0.9067\n",
            "Epoch 652/700\n",
            " - 0s - loss: 0.2646 - acc: 0.8950\n",
            "Epoch 653/700\n",
            " - 0s - loss: 0.2336 - acc: 0.9125\n",
            "Epoch 654/700\n",
            " - 0s - loss: 0.2333 - acc: 0.9096\n",
            "Epoch 655/700\n",
            " - 0s - loss: 0.2434 - acc: 0.9067\n",
            "Epoch 656/700\n",
            " - 0s - loss: 0.2612 - acc: 0.8980\n",
            "Epoch 657/700\n",
            " - 0s - loss: 0.2349 - acc: 0.9038\n",
            "Epoch 658/700\n",
            " - 0s - loss: 0.2501 - acc: 0.8950\n",
            "Epoch 659/700\n",
            " - 0s - loss: 0.2871 - acc: 0.8863\n",
            "Epoch 660/700\n",
            " - 0s - loss: 0.2342 - acc: 0.9067\n",
            "Epoch 661/700\n",
            " - 0s - loss: 0.2685 - acc: 0.8834\n",
            "Epoch 662/700\n",
            " - 0s - loss: 0.2407 - acc: 0.8980\n",
            "Epoch 663/700\n",
            " - 0s - loss: 0.2267 - acc: 0.9125\n",
            "Epoch 664/700\n",
            " - 0s - loss: 0.2380 - acc: 0.9125\n",
            "Epoch 665/700\n",
            " - 0s - loss: 0.2405 - acc: 0.8980\n",
            "Epoch 666/700\n",
            " - 0s - loss: 0.2185 - acc: 0.9155\n",
            "Epoch 667/700\n",
            " - 0s - loss: 0.2289 - acc: 0.9125\n",
            "Epoch 668/700\n",
            " - 0s - loss: 0.2305 - acc: 0.8980\n",
            "Epoch 669/700\n",
            " - 0s - loss: 0.2375 - acc: 0.9009\n",
            "Epoch 670/700\n",
            " - 0s - loss: 0.2428 - acc: 0.9067\n",
            "Epoch 671/700\n",
            " - 0s - loss: 0.2588 - acc: 0.8892\n",
            "Epoch 672/700\n",
            " - 0s - loss: 0.2859 - acc: 0.8863\n",
            "Epoch 673/700\n",
            " - 0s - loss: 0.2688 - acc: 0.8863\n",
            "Epoch 674/700\n",
            " - 0s - loss: 0.2642 - acc: 0.8980\n",
            "Epoch 675/700\n",
            " - 0s - loss: 0.2548 - acc: 0.8950\n",
            "Epoch 676/700\n",
            " - 0s - loss: 0.2223 - acc: 0.9213\n",
            "Epoch 677/700\n",
            " - 0s - loss: 0.2442 - acc: 0.9009\n",
            "Epoch 678/700\n",
            " - 0s - loss: 0.2473 - acc: 0.8863\n",
            "Epoch 679/700\n",
            " - 0s - loss: 0.2427 - acc: 0.9096\n",
            "Epoch 680/700\n",
            " - 0s - loss: 0.2425 - acc: 0.9009\n",
            "Epoch 681/700\n",
            " - 0s - loss: 0.2122 - acc: 0.9213\n",
            "Epoch 682/700\n",
            " - 0s - loss: 0.2007 - acc: 0.9242\n",
            "Epoch 683/700\n",
            " - 0s - loss: 0.2383 - acc: 0.9067\n",
            "Epoch 684/700\n",
            " - 0s - loss: 0.2130 - acc: 0.9184\n",
            "Epoch 685/700\n",
            " - 0s - loss: 0.2767 - acc: 0.8863\n",
            "Epoch 686/700\n",
            " - 0s - loss: 0.2408 - acc: 0.9096\n",
            "Epoch 687/700\n",
            " - 0s - loss: 0.2307 - acc: 0.9067\n",
            "Epoch 688/700\n",
            " - 0s - loss: 0.2294 - acc: 0.9155\n",
            "Epoch 689/700\n",
            " - 0s - loss: 0.2616 - acc: 0.8980\n",
            "Epoch 690/700\n",
            " - 0s - loss: 0.2299 - acc: 0.9009\n",
            "Epoch 691/700\n",
            " - 0s - loss: 0.2431 - acc: 0.9009\n",
            "Epoch 692/700\n",
            " - 0s - loss: 0.2708 - acc: 0.8892\n",
            "Epoch 693/700\n",
            " - 0s - loss: 0.2384 - acc: 0.8980\n",
            "Epoch 694/700\n",
            " - 0s - loss: 0.2611 - acc: 0.8950\n",
            "Epoch 695/700\n",
            " - 0s - loss: 0.2513 - acc: 0.9125\n",
            "Epoch 696/700\n",
            " - 0s - loss: 0.2293 - acc: 0.9096\n",
            "Epoch 697/700\n",
            " - 0s - loss: 0.2392 - acc: 0.9038\n",
            "Epoch 698/700\n",
            " - 0s - loss: 0.2641 - acc: 0.8776\n",
            "Epoch 699/700\n",
            " - 0s - loss: 0.2431 - acc: 0.9038\n",
            "Epoch 700/700\n",
            " - 0s - loss: 0.2485 - acc: 0.8950\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8b53cab358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_rNIwlLlFFW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eb3fac3f-425a-44f2-e690-8ad4fb00ee30"
      },
      "source": [
        "hist = model.fit(X_test, Y_test, epochs=100, batch_size=10, validation_split=0.1)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 152 samples, validate on 17 samples\n",
            "Epoch 1/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2723 - acc: 0.8947 - val_loss: 0.0724 - val_acc: 1.0000\n",
            "Epoch 2/100\n",
            "152/152 [==============================] - 0s 992us/step - loss: 0.3251 - acc: 0.8750 - val_loss: 0.0725 - val_acc: 1.0000\n",
            "Epoch 3/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3049 - acc: 0.8618 - val_loss: 0.0745 - val_acc: 1.0000\n",
            "Epoch 4/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2623 - acc: 0.8947 - val_loss: 0.0798 - val_acc: 1.0000\n",
            "Epoch 5/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2744 - acc: 0.8882 - val_loss: 0.0792 - val_acc: 1.0000\n",
            "Epoch 6/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2806 - acc: 0.9145 - val_loss: 0.0890 - val_acc: 1.0000\n",
            "Epoch 7/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2809 - acc: 0.9079 - val_loss: 0.0699 - val_acc: 1.0000\n",
            "Epoch 8/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2912 - acc: 0.8882 - val_loss: 0.0677 - val_acc: 1.0000\n",
            "Epoch 9/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2650 - acc: 0.8947 - val_loss: 0.0687 - val_acc: 1.0000\n",
            "Epoch 10/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2496 - acc: 0.9145 - val_loss: 0.0693 - val_acc: 1.0000\n",
            "Epoch 11/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2392 - acc: 0.9013 - val_loss: 0.0730 - val_acc: 1.0000\n",
            "Epoch 12/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2721 - acc: 0.8750 - val_loss: 0.0775 - val_acc: 1.0000\n",
            "Epoch 13/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2990 - acc: 0.8618 - val_loss: 0.1905 - val_acc: 0.9412\n",
            "Epoch 14/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2787 - acc: 0.8750 - val_loss: 0.1910 - val_acc: 0.9412\n",
            "Epoch 15/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3337 - acc: 0.8684 - val_loss: 0.1909 - val_acc: 0.9412\n",
            "Epoch 16/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3880 - acc: 0.7961 - val_loss: 0.1924 - val_acc: 0.9412\n",
            "Epoch 17/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3099 - acc: 0.8487 - val_loss: 0.1955 - val_acc: 0.9412\n",
            "Epoch 18/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2443 - acc: 0.8816 - val_loss: 0.1947 - val_acc: 0.9412\n",
            "Epoch 19/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3044 - acc: 0.8618 - val_loss: 0.1936 - val_acc: 0.9412\n",
            "Epoch 20/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2801 - acc: 0.8618 - val_loss: 0.1825 - val_acc: 0.9412\n",
            "Epoch 21/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3345 - acc: 0.8487 - val_loss: 0.1817 - val_acc: 0.9412\n",
            "Epoch 22/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2946 - acc: 0.8421 - val_loss: 0.1822 - val_acc: 0.9412\n",
            "Epoch 23/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2307 - acc: 0.9079 - val_loss: 0.1815 - val_acc: 0.9412\n",
            "Epoch 24/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3213 - acc: 0.8553 - val_loss: 0.1812 - val_acc: 0.9412\n",
            "Epoch 25/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2357 - acc: 0.9013 - val_loss: 0.1809 - val_acc: 0.9412\n",
            "Epoch 26/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3022 - acc: 0.8816 - val_loss: 0.1807 - val_acc: 0.9412\n",
            "Epoch 27/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3242 - acc: 0.8750 - val_loss: 0.1805 - val_acc: 0.9412\n",
            "Epoch 28/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2952 - acc: 0.8816 - val_loss: 0.1805 - val_acc: 0.9412\n",
            "Epoch 29/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3111 - acc: 0.8750 - val_loss: 0.1809 - val_acc: 0.9412\n",
            "Epoch 30/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2962 - acc: 0.8684 - val_loss: 0.1806 - val_acc: 0.9412\n",
            "Epoch 31/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2576 - acc: 0.8684 - val_loss: 0.1805 - val_acc: 0.9412\n",
            "Epoch 32/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2723 - acc: 0.8882 - val_loss: 0.1805 - val_acc: 0.9412\n",
            "Epoch 33/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2886 - acc: 0.8750 - val_loss: 0.1805 - val_acc: 0.9412\n",
            "Epoch 34/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2866 - acc: 0.8816 - val_loss: 0.1806 - val_acc: 0.9412\n",
            "Epoch 35/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2863 - acc: 0.8816 - val_loss: 0.1802 - val_acc: 0.9412\n",
            "Epoch 36/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2445 - acc: 0.8882 - val_loss: 0.1804 - val_acc: 0.9412\n",
            "Epoch 37/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2449 - acc: 0.8947 - val_loss: 0.1803 - val_acc: 0.9412\n",
            "Epoch 38/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2803 - acc: 0.8750 - val_loss: 0.1802 - val_acc: 0.9412\n",
            "Epoch 39/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3064 - acc: 0.8487 - val_loss: 0.1803 - val_acc: 0.9412\n",
            "Epoch 40/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2854 - acc: 0.8750 - val_loss: 0.1803 - val_acc: 0.9412\n",
            "Epoch 41/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3318 - acc: 0.8487 - val_loss: 0.1807 - val_acc: 0.9412\n",
            "Epoch 42/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2937 - acc: 0.8618 - val_loss: 0.1810 - val_acc: 0.9412\n",
            "Epoch 43/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2817 - acc: 0.8947 - val_loss: 0.1805 - val_acc: 0.9412\n",
            "Epoch 44/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2971 - acc: 0.8750 - val_loss: 0.1801 - val_acc: 0.9412\n",
            "Epoch 45/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2673 - acc: 0.8882 - val_loss: 0.1802 - val_acc: 0.9412\n",
            "Epoch 46/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2384 - acc: 0.8947 - val_loss: 0.1802 - val_acc: 0.9412\n",
            "Epoch 47/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2811 - acc: 0.8947 - val_loss: 0.1804 - val_acc: 0.9412\n",
            "Epoch 48/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3029 - acc: 0.8618 - val_loss: 0.1804 - val_acc: 0.9412\n",
            "Epoch 49/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2938 - acc: 0.8684 - val_loss: 0.1805 - val_acc: 0.9412\n",
            "Epoch 50/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2862 - acc: 0.8750 - val_loss: 0.1805 - val_acc: 0.9412\n",
            "Epoch 51/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2989 - acc: 0.8618 - val_loss: 0.1807 - val_acc: 0.9412\n",
            "Epoch 52/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2979 - acc: 0.8618 - val_loss: 0.1807 - val_acc: 0.9412\n",
            "Epoch 53/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3096 - acc: 0.8684 - val_loss: 0.1810 - val_acc: 0.9412\n",
            "Epoch 54/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3087 - acc: 0.8553 - val_loss: 0.1810 - val_acc: 0.9412\n",
            "Epoch 55/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3179 - acc: 0.8618 - val_loss: 0.1819 - val_acc: 0.9412\n",
            "Epoch 56/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2834 - acc: 0.8618 - val_loss: 0.1834 - val_acc: 0.9412\n",
            "Epoch 57/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3019 - acc: 0.8684 - val_loss: 0.1832 - val_acc: 0.9412\n",
            "Epoch 58/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3025 - acc: 0.8684 - val_loss: 0.1829 - val_acc: 0.9412\n",
            "Epoch 59/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2538 - acc: 0.8750 - val_loss: 0.1832 - val_acc: 0.9412\n",
            "Epoch 60/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2635 - acc: 0.8882 - val_loss: 0.1825 - val_acc: 0.9412\n",
            "Epoch 61/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3362 - acc: 0.8421 - val_loss: 0.1826 - val_acc: 0.9412\n",
            "Epoch 62/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2471 - acc: 0.8947 - val_loss: 0.1819 - val_acc: 0.9412\n",
            "Epoch 63/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3496 - acc: 0.8421 - val_loss: 0.1808 - val_acc: 0.9412\n",
            "Epoch 64/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2964 - acc: 0.8684 - val_loss: 0.1811 - val_acc: 0.9412\n",
            "Epoch 65/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3102 - acc: 0.8421 - val_loss: 0.1809 - val_acc: 0.9412\n",
            "Epoch 66/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2909 - acc: 0.8618 - val_loss: 0.1814 - val_acc: 0.9412\n",
            "Epoch 67/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2879 - acc: 0.8618 - val_loss: 0.1810 - val_acc: 0.9412\n",
            "Epoch 68/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3043 - acc: 0.8618 - val_loss: 0.1809 - val_acc: 0.9412\n",
            "Epoch 69/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2863 - acc: 0.8618 - val_loss: 0.1807 - val_acc: 0.9412\n",
            "Epoch 70/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3087 - acc: 0.8553 - val_loss: 0.1812 - val_acc: 0.9412\n",
            "Epoch 71/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2843 - acc: 0.8816 - val_loss: 0.1812 - val_acc: 0.9412\n",
            "Epoch 72/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2660 - acc: 0.8882 - val_loss: 0.1802 - val_acc: 0.9412\n",
            "Epoch 73/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2618 - acc: 0.8816 - val_loss: 0.1798 - val_acc: 0.9412\n",
            "Epoch 74/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3046 - acc: 0.8487 - val_loss: 0.1798 - val_acc: 0.9412\n",
            "Epoch 75/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2714 - acc: 0.8947 - val_loss: 0.1796 - val_acc: 0.9412\n",
            "Epoch 76/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3111 - acc: 0.8684 - val_loss: 0.1795 - val_acc: 0.9412\n",
            "Epoch 77/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2969 - acc: 0.8684 - val_loss: 0.1800 - val_acc: 0.9412\n",
            "Epoch 78/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2900 - acc: 0.8421 - val_loss: 0.1805 - val_acc: 0.9412\n",
            "Epoch 79/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2996 - acc: 0.8684 - val_loss: 0.1811 - val_acc: 0.9412\n",
            "Epoch 80/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2876 - acc: 0.8750 - val_loss: 0.1810 - val_acc: 0.9412\n",
            "Epoch 81/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2893 - acc: 0.8750 - val_loss: 0.1806 - val_acc: 0.9412\n",
            "Epoch 82/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2789 - acc: 0.8816 - val_loss: 0.1805 - val_acc: 0.9412\n",
            "Epoch 83/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3004 - acc: 0.8618 - val_loss: 0.1801 - val_acc: 0.9412\n",
            "Epoch 84/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2720 - acc: 0.8750 - val_loss: 0.1799 - val_acc: 0.9412\n",
            "Epoch 85/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3090 - acc: 0.8684 - val_loss: 0.1799 - val_acc: 0.9412\n",
            "Epoch 86/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2668 - acc: 0.8816 - val_loss: 0.1797 - val_acc: 0.9412\n",
            "Epoch 87/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2498 - acc: 0.8947 - val_loss: 0.1796 - val_acc: 0.9412\n",
            "Epoch 88/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3219 - acc: 0.8816 - val_loss: 0.1795 - val_acc: 0.9412\n",
            "Epoch 89/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2586 - acc: 0.8750 - val_loss: 0.1797 - val_acc: 0.9412\n",
            "Epoch 90/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3431 - acc: 0.8421 - val_loss: 0.1799 - val_acc: 0.9412\n",
            "Epoch 91/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2905 - acc: 0.8684 - val_loss: 0.1800 - val_acc: 0.9412\n",
            "Epoch 92/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2969 - acc: 0.8618 - val_loss: 0.1799 - val_acc: 0.9412\n",
            "Epoch 93/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2854 - acc: 0.8882 - val_loss: 0.1799 - val_acc: 0.9412\n",
            "Epoch 94/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3338 - acc: 0.8553 - val_loss: 0.1799 - val_acc: 0.9412\n",
            "Epoch 95/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2865 - acc: 0.8355 - val_loss: 0.1800 - val_acc: 0.9412\n",
            "Epoch 96/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2114 - acc: 0.9013 - val_loss: 0.1800 - val_acc: 0.9412\n",
            "Epoch 97/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2346 - acc: 0.8882 - val_loss: 0.1798 - val_acc: 0.9412\n",
            "Epoch 98/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.2608 - acc: 0.8816 - val_loss: 0.1798 - val_acc: 0.9412\n",
            "Epoch 99/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3234 - acc: 0.8553 - val_loss: 0.1798 - val_acc: 0.9412\n",
            "Epoch 100/100\n",
            "152/152 [==============================] - 0s 1ms/step - loss: 0.3177 - acc: 0.8553 - val_loss: 0.1800 - val_acc: 0.9412\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNjlJPerEIv3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cdba8c52-b551-460f-d00d-3c3597506b20"
      },
      "source": [
        "import numpy as np\n",
        "print (\"Training Accuracy\", np.mean(hist.history['acc']))\n",
        "print (\"Validation Accuracy\", np.mean(hist.history['val_acc']))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy 0.8724999938944452\n",
            "Validation Accuracy 0.948235297203064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4ebd7bc1-53c0-0e31-a0b0-b6d0a3017434",
        "_uuid": "47e99d7ed1f27a85eb01dbafc71b66b329fb1d12",
        "id": "B3UNa5RUDrQH",
        "colab_type": "text"
      },
      "source": [
        "Extracting a validation set, and measuring score and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWfVesRFO8lH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "340a0ef8-be58-48d9-f61a-130aa41d2115"
      },
      "source": [
        ""
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "a970f412-722f-6d6d-72c8-325d0901ccef",
        "_uuid": "7872f6ea819a5d4d08394ba6db8436f9cb2cfe1c",
        "trusted": false,
        "id": "jaZsRY9yDrQI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "663f12c2-e340-4d3a-d724-0b897fd42a9c"
      },
      "source": [
        "validation_size = 1500\n",
        "\n",
        "X_validate = X_test[-validation_size:]\n",
        "Y_validate = Y_test[-validation_size:]\n",
        "X_test = X_test[:-validation_size]\n",
        "Y_test = Y_test[:-validation_size]\n",
        "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
        "print(\"score: %.2f\" % (score))\n",
        "print(\"acc: %.2f\" % (acc))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-1e54ca793183>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mvalidation_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mvalidation_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"score: %.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"acc: %.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "018ebf39-9414-27d0-232c-a34de051feaf",
        "_uuid": "4b54f18bbf22a953c60f271c318cb076e684df9c",
        "id": "xNVOLvIzDrQM",
        "colab_type": "text"
      },
      "source": [
        "Finally measuring the number of correct guesses.  It is clear that finding negative tweets goes very well for the Network but deciding whether is positive is not really. My educated guess here is that the positive training set is dramatically smaller than the negative, hence the \"bad\" results for positive tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "1add73e9-c6fb-7e4c-8715-ea92f519d2a6",
        "_uuid": "f80e9f3cf281adb3ab0357cbf6f886eb1dce3005",
        "trusted": false,
        "id": "oQ7MsSt4DrQM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a5c153ab-293c-4a96-9c72-259561088616"
      },
      "source": [
        "pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n",
        "for x in range(len(X_validate)):\n",
        "    \n",
        "    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
        "   \n",
        "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
        "        if np.argmax(Y_validate[x]) == 0:\n",
        "            neg_correct += 1\n",
        "        else:\n",
        "            pos_correct += 1\n",
        "       \n",
        "    if np.argmax(Y_validate[x]) == 0:\n",
        "        neg_cnt += 1\n",
        "    else:\n",
        "        pos_cnt += 1\n",
        "\n",
        "\n",
        "\n",
        "print(\"pos_acc\", pos_correct/pos_cnt*100, \"%\")\n",
        "print(\"neg_acc\", neg_correct/neg_cnt*100, \"%\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos_acc 98.73417721518987 %\n",
            "neg_acc 86.66666666666667 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "890a03c9-316e-4d55-98e1-ba29045eff6c",
        "_uuid": "cfcbefe939b72297019e221ca3f5a283974bffff",
        "id": "FPH3W54LDrQS",
        "colab_type": "text"
      },
      "source": [
        "As it was requested by the crowd, I extended the kernel with a prediction example, and also updated the API calls to Keras 2.0. Please note that the network performs poorly. Its because the training data is very unbalanced (pos: 4472, neg: 16986), you should get more data, use other dataset, use pre-trained model, or weight classes to achieve reliable predictions.\n",
        "\n",
        "I have created this kernel when I knew much less about LSTM & ML. It is a really basic, beginner level kernel, yet it had a huge audience in the past year. I had a lot of private questions and requests regarding this notebook and I tried my best to help and answer them . In the future I am not planning to answer custom questions and support/enhance this kernel in any ways. Thank you my folks :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "24c64f46-edd1-8d0b-7c7c-ef50fd26b2fd",
        "_uuid": "d9aac68e2013b3beffb6a764cc5b85be83073e66",
        "trusted": false,
        "id": "3wbEJztnDrQT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "86388baf-d005-4b62-d465-f1fe33a75597"
      },
      "source": [
        "twt = ['Meetings: Because none of us is as dumb as all of us.']\n",
        "#vectorizing the tweet by the pre-fitted tokenizer instance\n",
        "twt = tokenizer.texts_to_sequences(twt)\n",
        "#padding the tweet to have exactly the same shape as `embedding_2` input\n",
        "twt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)\n",
        "print(twt)\n",
        "sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n",
        "if(np.argmax(sentiment) == 0):\n",
        "    print(\"negative\")\n",
        "elif (np.argmax(sentiment) == 1):\n",
        "    print(\"positive\")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-85a7e231d035>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtwt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m         \u001b[0;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    139\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_4_input to have shape (210430,) but got array with shape (28,)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "c611b55c-92e4-4a33-8e82-1812bef6193d",
        "_uuid": "8b10995b0832ec98ba0c75832186fcb09b1a2d5f",
        "trusted": false,
        "id": "GjGGQMuRDrQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "",
        "_uuid": "",
        "trusted": true,
        "id": "2lP5MoaLDrQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-FNNryOnZlU",
        "colab_type": "text"
      },
      "source": [
        "# CNN\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgi8VrWWnkX1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9135b59d-a5a5-4e91-977a-2755d54fe6da"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(343, 210430)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx_L2EvSoV-v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "cf7308d4-55a1-46f2-fdfb-e6f3f048b49c"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "print(X_train.shape)\n",
        "#x_train = np.reshape(X_train, (X_train.shape[0],5, 5,1))\n",
        "padie=np.pad(X_train,  ((0,0),(0,251)), 'constant', constant_values=0)\n",
        "x1_train = np.reshape(padie, (padie.shape[0],459, 459,1))\n",
        "print(x1_train.shape)\n",
        "print(Y_train.shape[0])\n",
        "#x_test = np.reshape(x_test, (x_test.shape[0],2,2, 1))\n",
        "\n",
        "padise=np.pad(X_test,  ((0,0),(0,251)), 'constant', constant_values=0)\n",
        "xtest = np.reshape(padise, (padise.shape[0],459, 459,1))\n",
        "#print(xtest.shape)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(343, 210430)\n",
            "(343, 459, 459, 1)\n",
            "343\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kwm-Mhgjo3Bq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "0d89e2a0-9830-4d65-e62c-cfd33e91f660"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "DrawMwe = plt.imshow(x1_train[4][:,:,0])  #[Index Of Number][Y,X,Channel]"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAOsklEQVR4nO3dXYxc9X3G8e+zM97ZeDE2NpZr7W5i\nB5wgmjYYrcCURk1BpIZGMRckAqFiRZZ8QyUiIqXQSq0i9SLcxAlShWrVKE4VBSiJhItQkWOIolzw\nYgIhgGVYaKjtGlzAdnj1y+6vF+e/y3hZ2PXujGdWv+cjjeb/ds75ndn143NmRlpFBGaWV0+nCzCz\nznIImCXnEDBLziFglpxDwCw5h4BZcm0JAUnrJe2VNCLptnYcw8xaQ63+noCkGvAicBWwH3gSuCEi\nXmjpgcysJdpxJXAJMBIRr0TEceAeYEMbjmNmLVBvwz4HgH1N/f3ApZ+0Qa8a0Ud/G0oxs3Fvc/iN\niFg+ebwdITAjkjYDmwH6WMilurJTpZil8Iu4/9WpxttxO3AAGGrqD5axU0TE1ogYjojhBTTaUIaZ\nzUQ7QuBJYI2k1ZJ6geuBHW04jpm1QMtvByLipKS/BR4GasDdEfF8q49jZq3RlvcEIuIh4KF27NvM\nWsvfGDRLziFglpxDwCw5h4BZcg4Bs+QcAmbJOQTMknMImCXnEDBLziFglpxDwCw5h4BZcg4Bs+Qc\nAmbJOQTMknMImCXnEDBLziFglpxDwCw5h4BZcg4Bs+QcAmbJOQTMknMImCXnEDBLziFglpxDwCw5\nh4BZcg4Bs+QcAmbJOQTMknMImCXnEDBLziFglpxDwCw5h4BZcg4Bs+QcAmbJOQTMkps2BCTdLemQ\npOeaxpZK2inppfJ8ThmXpDsljUh6VtLF7SzezOZuJlcCPwLWTxq7DdgVEWuAXaUPcDWwpjw2A3e1\npkwza5dpQyAifgW8NWl4A7C9tLcD1zaN/zgqjwFLJK1sVbFm1nqzfU9gRUQcLO3XgBWlPQDsa1q3\nv4x9hKTNknZL2n2CY7Msw8zmas5vDEZEADGL7bZGxHBEDC+gMdcyzGyWZhsCr49f5pfnQ2X8ADDU\ntG6wjJlZl5ptCOwANpb2RuCBpvGbyqcE64CjTbcNZtaF6tMtkPRT4MvAuZL2A/8EfA+4T9Im4FXg\nG2X5Q8A1wAjwHvDNNtRsZi00bQhExA0fM3XlFGsDuHmuRZnZmeNvDJol5xAwS84hYJacQ8AsOYeA\nWXIOAbPkHAJmyTkEzJJzCJgl5xAwS84hYJacQ8AsOYeAWXIOAbPkHAJmyTkEzJJzCJgl5xAwS84h\nYJacQ8AsOYeAWXIOAbPkHAJmyTkEzJJzCJgl5xAwS84hYJacQ8AsOYeAWXIOAbPkHAJmyTkEzJJz\nCJgl5xAwS84hYJacQ8AsOYeAWXIOAbPkpg0BSUOSHpX0gqTnJd1SxpdK2inppfJ8ThmXpDsljUh6\nVtLF7T4JM5u9mVwJnAS+HREXAuuAmyVdCNwG7IqINcCu0ge4GlhTHpuBu1petZm1zLQhEBEHI+I3\npf02sAcYADYA28uy7cC1pb0B+HFUHgOWSFrZ8srNrCVO6z0BSauAtcDjwIqIOFimXgNWlPYAsK9p\ns/1lbPK+NkvaLWn3CY6dZtlm1iozDgFJZwE/A74VEX9onouIAOJ0DhwRWyNiOCKGF9A4nU3NrIVm\nFAKSFlAFwE8i4udl+PXxy/zyfKiMHwCGmjYfLGNm1oVm8umAgG3Anoj4ftPUDmBjaW8EHmgav6l8\nSrAOONp022BmXaY+gzWXA38D/E7SM2Xs74HvAfdJ2gS8CnyjzD0EXAOMAO8B32xpxWbWUtOGQET8\nGtDHTF85xfoAbp5jXWZ2hvgbg2bJOQTMknMImCXnEDBLziFglpxDwCw5h4BZcg4Bs+QcAmbJOQTM\nknMImCXnEDBLziFglpxDwCw5h4BZcg4Bs+QcAmbJOQTMknMImCXnEDBLziFglpxDwCw5h4BZcg4B\ns+QcAmbJOQTMknMImCXnEDBLziFglpxDwCw5h4BZcg4Bs+QcAmbJOQTMknMImCXnEDBLziFglpxD\nwCy5aUNAUp+kJyT9VtLzkr5bxldLelzSiKR7JfWW8Ubpj5T5Ve09BTObi5lcCRwDroiILwIXAesl\nrQPuALZExPnAYWBTWb8JOFzGt5R1Ztalpg2BqLxTugvKI4ArgPvL+Hbg2tLeUPqU+SslqWUVm1lL\nzeg9AUk1Sc8Ah4CdwMvAkYg4WZbsBwZKewDYB1DmjwLLWlm0mbXOjEIgIkYj4iJgELgEuGCuB5a0\nWdJuSbtPcGyuuzOzWTqtTwci4gjwKHAZsERSvUwNAgdK+wAwBFDmFwNvTrGvrRExHBHDC2jMsnwz\nm6uZfDqwXNKS0v4UcBWwhyoMrivLNgIPlPaO0qfMPxIR0cqizax16tMvYSWwXVKNKjTui4gHJb0A\n3CPpn4GngW1l/Tbg3yWNAG8B17ehbjNrkWlDICKeBdZOMf4K1fsDk8c/AL7ekurMrO38jUGz5BwC\nZsk5BMyScwiYJecQMEvOIWCWnEPALDmHgFlyDgGz5BwCZsk5BMyScwiYJecQMEvOIWCWnEPALDmH\ngFlyDgGz5BwCZsk5BMyScwiYJecQMEvOIWCWnEPALDmHgFlyDgGz5BwCZsk5BMyScwiYJecQMEvO\nIWCWnEPALDmHgFlyDgGz5BwCZsk5BMyScwiYJecQMEvOIWCWnEPALLkZh4CkmqSnJT1Y+qslPS5p\nRNK9knrLeKP0R8r8qvaUbmatcDpXArcAe5r6dwBbIuJ84DCwqYxvAg6X8S1lnZl1qRmFgKRB4K+B\nfyt9AVcA95cl24FrS3tD6VPmryzrzawLzfRK4AfAd4Cx0l8GHImIk6W/Hxgo7QFgH0CZP1rWn0LS\nZkm7Je0+wbFZlm9mczVtCEj6KnAoIp5q5YEjYmtEDEfE8AIardy1mZ2G+gzWXA58TdI1QB9wNvBD\nYImkevnffhA4UNYfAIaA/ZLqwGLgzZZXbmYtMe2VQETcHhGDEbEKuB54JCJuBB4FrivLNgIPlPaO\n0qfMPxIR0dKqzaxl5vI9gb8DbpU0QnXPv62MbwOWlfFbgdvmVqKZtdNMbgcmRMQvgV+W9ivAJVOs\n+QD4egtqM7MzwN8YNEvOIWCWnEPALDmHgFlyDgGz5BwCZsk5BMyScwiYJecQMEvOIWCWnEPALDmH\ngFlyDgGz5BwCZsk5BMyScwiYJecQMEvOIWCWnEPALDmHgFlyDoF5QI0GcflFqF5HjcZH+lO2P2nu\nNNdNu49p1tU+dx5IVfvss6d8IH24Zslievr7qS1bSm35clSv09PXx9iX1oJET18ftWVLJ7Zp7k+s\n/Yu1E/P1ocGJdWNfWovqdZCo/fHnJ9rj8+OP8fHx/TWPN/fHt0Wq6i5rVK9TW7L4w7lynuPbq16v\nznHJ4mrt+as5/lfD1IcG6envn9jfeB0Hb/0z6qs+Xa1dtpT60CC1z51HfeUfUVu+nPrgwMRYT39/\ntf8vXEB91acnXv+P/f3qhj8JsLh+bqzrvRr1L2T0rcMA1BYtgqGVjL7wIgA9jQZatIjRN95Avb0w\nOsqR64c555kj8N/7UF8DRkcZe/8DACRVY+cuZez3+4jR0WqfAEMrGdv7MtRqxPHj1b7LsXsa1V9D\nGj9WT6MBtRqMjqL+hcS77xEnT05sW1u0CAZWEP/zv2hBnbF33q22r9dPOX7PmtVw4PVPPP7YO+/C\n6Cg9nz+Psb0vE6Oj9DQajB07Rm3x2R/u66x+xt55tzpGby9j771Xnd/is08Zj+PHJ2p+/y//hMZ/\n7Ua12sT59JzVz9j7H3x4HjCxzSk/hwX1KV+b8WP0LFxYnfv7H8DoKNRq1etfapt4rS44Hw68PvUx\nys9avb3Vtud95pTXYPw1b95/RJx6HPjosZtNmht/nVWv8383rmXZ3Y9V51deI9Xr1bqFC3nnK1+g\n/z+fmqhHfY2P/KzH+zE6imq16nn851D2+Umvw8S5N/0uzuR1mDjGNK/DzhP3PBURw5P//XVFCEh6\nG9jb6Trm4FzgjU4XMUuuvXPOdP2fiYjlkwdP6+8OtNHeqRJqvpC0e77W79o7p1vq93sCZsk5BMyS\n65YQ2NrpAuZoPtfv2junK+rvijcGzaxzuuVKwMw6pOMhIGm9pL2SRiR13Z8xl3S3pEOSnmsaWypp\np6SXyvM5ZVyS7izn8qykiztXOUgakvSopBckPS/plnlWf5+kJyT9ttT/3TK+WtLjpc57JfWW8Ubp\nj5T5VZ2sv9RUk/S0pAdLv+tq72gISKoB/wJcDVwI3CDpwk7WNIUfAesnjd0G7IqINcCu0ofqPNaU\nx2bgrjNU48c5CXw7Ii4E1gE3l9d3vtR/DLgiIr4IXASsl7QOuAPYEhHnA4eBTWX9JuBwGd9S1nXa\nLcCepn731R4RHXsAlwEPN/VvB27vZE0fU+cq4Lmm/l5gZWmvpPqeA8C/AjdMta4bHsADwFXzsX5g\nIfAb4FKqL9jUJ/8OAQ8Dl5V2vaxTB2sepArZK4AHAXVj7Z2+HRgA9jX195exbrciIg6W9mvAitLu\n2vMpl5drgceZR/WXy+lngEPATuBl4EhEnCxLmmucqL/MHwWWndmKT/ED4DvAWOkvowtr73QIzHtR\nRXdXf8Qi6SzgZ8C3IuIPzXPdXn9EjEbERVT/q14CXNDhkmZE0leBQxHxVKdrmU6nQ+AAMNTUHyxj\n3e51SSsByvOhMt515yNpAVUA/CQifl6G50394yLiCPAo1SX0EknjX3lvrnGi/jK/GHjzDJc67nLg\na5J+D9xDdUvwQ7qw9k6HwJPAmvKOaS9wPbCjwzXNxA5gY2lvpLrXHh+/qbzLvg442nTZfcZJErAN\n2BMR32+ami/1L5e0pLQ/RfV+xh6qMLiuLJtc//h5XQc8Uq50zriIuD0iBiNiFdXv9SMRcSPdWHsX\nvOFzDfAi1b3eP3S6ninq+ylwEDhBdQ+3iepebRfwEvALYGlZK6pPO14GfgcMd7j2P6e61H8WeKY8\nrplH9f8p8HSp/zngH8v4Z4EngBHgP4BGGe8r/ZEy/9lO//6Uur4MPNittfsbg2bJdfp2wMw6zCFg\nlpxDwCw5h4BZcg4Bs+QcAmbJOQTMknMImCX3/xmlEZfa4HnVAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GetyS0znpJfK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "import numpy as np\n",
        "from keras.optimizers import RMSprop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k2it_3EpgLh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier = Sequential()\n",
        "\n",
        "# Step 1 - Convolution\n",
        "classifier.add(Conv2D(32, (3, 3), input_shape = (459, 459, 1), activation = 'relu'))\n",
        "\n",
        "# Step 2 - Pooling\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# Adding a second convolutional layer\n",
        "classifier.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# Adding a third convolutional layer\n",
        "classifier.add(Conv2D(128, (3, 3), activation = 'relu'))\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "\n",
        "# Step 3 - Flattening\n",
        "classifier.add(Flatten())\n",
        "\n",
        "# Step 4 - Full connection\n",
        "classifier.add(Dense(units = 64, activation = 'relu'))\n",
        "classifier.add(Dense(units = 2, activation = 'sigmoid'))\n",
        "\n",
        "# Compiling the CNN\n",
        "opt = RMSprop(lr=1e-4)\n",
        "classifier.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8mpx1atpjtK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "823a1a8f-edd2-4162-d4c8-ba18bf973718"
      },
      "source": [
        "classifier.summary()\n",
        "classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "classifier.fit(x1_train, Y_train, epochs=100, batch_size=10)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 457, 457, 32)      320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 228, 228, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 226, 226, 64)      18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 113, 113, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 111, 111, 128)     73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 55, 55, 128)       0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 387200)            0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 64)                24780864  \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 2)                 130       \n",
            "=================================================================\n",
            "Total params: 24,873,666\n",
            "Trainable params: 24,873,666\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "343/343 [==============================] - 7s 22ms/step - loss: 1.9499 - acc: 0.7638\n",
            "Epoch 2/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.6714 - acc: 0.8382\n",
            "Epoch 3/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.2722 - acc: 0.9038\n",
            "Epoch 4/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.2223 - acc: 0.9169\n",
            "Epoch 5/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1806 - acc: 0.9300\n",
            "Epoch 6/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1760 - acc: 0.9300\n",
            "Epoch 7/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1706 - acc: 0.9300\n",
            "Epoch 8/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1737 - acc: 0.9300\n",
            "Epoch 9/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1684 - acc: 0.9300\n",
            "Epoch 10/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1687 - acc: 0.9300\n",
            "Epoch 11/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1687 - acc: 0.9315\n",
            "Epoch 12/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1650 - acc: 0.9315\n",
            "Epoch 13/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1707 - acc: 0.9300\n",
            "Epoch 14/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1718 - acc: 0.9300\n",
            "Epoch 15/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1649 - acc: 0.9300\n",
            "Epoch 16/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1680 - acc: 0.9300\n",
            "Epoch 17/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1717 - acc: 0.9300\n",
            "Epoch 18/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1641 - acc: 0.9300\n",
            "Epoch 19/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1660 - acc: 0.9315\n",
            "Epoch 20/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1626 - acc: 0.9300\n",
            "Epoch 21/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1629 - acc: 0.9300\n",
            "Epoch 22/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1715 - acc: 0.9300\n",
            "Epoch 23/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1680 - acc: 0.9300\n",
            "Epoch 24/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1675 - acc: 0.9300\n",
            "Epoch 25/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1735 - acc: 0.9300\n",
            "Epoch 26/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1802 - acc: 0.9300\n",
            "Epoch 27/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1630 - acc: 0.9300\n",
            "Epoch 28/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1648 - acc: 0.9300\n",
            "Epoch 29/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1659 - acc: 0.9300\n",
            "Epoch 30/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1638 - acc: 0.9300\n",
            "Epoch 31/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1658 - acc: 0.9300\n",
            "Epoch 32/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1638 - acc: 0.9300\n",
            "Epoch 33/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1621 - acc: 0.9300\n",
            "Epoch 34/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1617 - acc: 0.9300\n",
            "Epoch 35/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1637 - acc: 0.9300\n",
            "Epoch 36/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1609 - acc: 0.9300\n",
            "Epoch 37/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1617 - acc: 0.9300\n",
            "Epoch 38/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1683 - acc: 0.9300\n",
            "Epoch 39/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1636 - acc: 0.9300\n",
            "Epoch 40/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1693 - acc: 0.9300\n",
            "Epoch 41/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1629 - acc: 0.9300\n",
            "Epoch 42/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1633 - acc: 0.9300\n",
            "Epoch 43/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1661 - acc: 0.9300\n",
            "Epoch 44/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1640 - acc: 0.9300\n",
            "Epoch 45/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1639 - acc: 0.9300\n",
            "Epoch 46/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1624 - acc: 0.9300\n",
            "Epoch 47/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1620 - acc: 0.9300\n",
            "Epoch 48/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1617 - acc: 0.9300\n",
            "Epoch 49/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1628 - acc: 0.9300\n",
            "Epoch 50/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1620 - acc: 0.9300\n",
            "Epoch 51/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1631 - acc: 0.9300\n",
            "Epoch 52/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1607 - acc: 0.9300\n",
            "Epoch 53/100\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.1621 - acc: 0.9300\n",
            "Epoch 54/100\n",
            "240/343 [===================>..........] - ETA: 0s - loss: 0.1930 - acc: 0.9125"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-332eeaad0179>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}